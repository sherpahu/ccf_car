{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SALES_DATA_PATH = \"../data/train_sales_data.csv\"\n",
    "TRAIN_SEARCH_DATA_PATH = \"../data/train_search_data.csv\"\n",
    "TRAIN_USER_REPLY_DATA_PATH = \"../data/train_user_reply_data.csv\"\n",
    "TEST_PATH = \"../data/evaluation_public.csv\"\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_data(df, data_type):\n",
    "    if data_type == \"train\":\n",
    "        return df[df.regYear == 2016]\n",
    "    elif data_type == \"test\":\n",
    "        return df[df.regYear == 2017]\n",
    "\n",
    "\n",
    "\n",
    "train_sales = pd.read_csv(TRAIN_SALES_DATA_PATH)\n",
    "train_search = pd.read_csv(TRAIN_SEARCH_DATA_PATH)\n",
    "train_user = pd.read_csv(TRAIN_USER_REPLY_DATA_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_sales.salesVolume = train_sales.salesVolume.apply(lambda x: np.log(1+x))\n",
    "\n",
    "# 特征工程\n",
    "def cal_basic_fea(df:pd.DataFrame, cal_col:str, stat_dim:list, data_type:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    计算原始特征、周期特征、趋势特征\n",
    "    \"\"\"\n",
    "    train_sales_data = get_data(train_sales, data_type)\n",
    "\n",
    "    name_prefix = \"_\".join(stat_dim) + \"_%s\"%cal_col\n",
    "    drop_name = \"level_%d\"%len(stat_dim)\n",
    "\n",
    "    # 原始特征\n",
    "    feature_data = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).unstack(level=-1)\n",
    "    feature_data.columns = [name_prefix + \"_%d\"%x for x in feature_data.columns.ravel()]\n",
    "    feature_data = feature_data.reset_index()\n",
    "\n",
    "    # 周期特征\n",
    "    ## shift_div\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).apply(lambda x: x / x.shift(1)).reset_index()\n",
    "\n",
    "    tmp_df = tmp_df.rename(columns={\"salesVolume\":\"shift_div\"})\n",
    "\n",
    "    train_sales_data = pd.merge(train_sales_data, tmp_df, on=stat_dim, how=\"left\")\n",
    "\n",
    "    tmp_df = train_sales_data.dropna().groupby(stat_dim).shift_div.apply(lambda x: x.sum()).unstack(level=-1)\n",
    "\n",
    "    tmp_df.columns = [name_prefix + \"_shift_div_%d\"%x for x in tmp_df.columns.ravel()]\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "\n",
    "    ## shift_sub\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).apply(lambda x: x - x.shift(1)).reset_index()\n",
    "\n",
    "    tmp_df = tmp_df.rename(columns={\"salesVolume\":\"shift_sub\"})\n",
    "\n",
    "    train_sales_data = pd.merge(train_sales_data, tmp_df, on=stat_dim, how=\"left\")\n",
    "\n",
    "    tmp_df = train_sales_data.dropna().groupby(stat_dim).shift_sub.apply(lambda x: x.sum()).unstack(level=-1)\n",
    "\n",
    "    tmp_df.columns = [name_prefix + \"_shift_sub_%d\"%x for x in tmp_df.columns.ravel()]\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "\n",
    "    # 趋势特征\n",
    "    ## shift_div\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[\"shift_div\"].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).apply(lambda x: x / x.shift(1)).reset_index()\n",
    "\n",
    "    tmp_df = tmp_df.rename(columns={\"shift_div\":\"shift_2_div\"})\n",
    "\n",
    "    train_sales_data = pd.merge(train_sales_data, tmp_df, on=stat_dim, how=\"left\")\n",
    "\n",
    "    tmp_df = train_sales_data.dropna().groupby(stat_dim).shift_div.apply(lambda x: x.sum()).unstack(level=-1)\n",
    "\n",
    "    tmp_df.columns = [name_prefix + \"_shift_2_div_%d\"%x for x in tmp_df.columns.ravel()]\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "\n",
    "    ## shift_sub\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[\"shift_sub\"].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).apply(lambda x: x - x.shift(1)).reset_index()\n",
    "\n",
    "    tmp_df = tmp_df.rename(columns={\"shift_sub\":\"shift_2_sub\"})\n",
    "\n",
    "    train_sales_data = pd.merge(train_sales_data, tmp_df, on=stat_dim, how=\"left\")\n",
    "\n",
    "    tmp_df = train_sales_data.dropna().groupby(stat_dim).shift_sub.apply(lambda x: x.sum()).unstack(level=-1)\n",
    "\n",
    "    tmp_df.columns = [name_prefix + \"_shift_2_sub_%d\"%x for x in tmp_df.columns.ravel()]\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "    return feature_data\n",
    "\n",
    "def cal_windows_fea(df:pd.DataFrame, cal_col:str, stat_dim:list, data_type:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    计算滑窗特征\n",
    "    \"\"\"\n",
    "    train_sales_data = get_data(df, data_type)\n",
    "\n",
    "    name_prefix = \"_\".join(stat_dim) + \"_%s\"%cal_col\n",
    "\n",
    "    # 滑窗特征\n",
    "    ## 均值\n",
    "    feature_data = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).rolling(3).mean()\n",
    "\n",
    "    feature_data = feature_data.dropna().unstack(level=-1)\n",
    "\n",
    "    if len(stat_dim) == 3:\n",
    "        feature_data.index = feature_data.index.droplevel(0)\n",
    "        feature_data.index = feature_data.index.droplevel(0)\n",
    "    elif len(stat_dim) == 2:\n",
    "        feature_data.index = feature_data.index.droplevel(0)\n",
    "\n",
    "\n",
    "    feature_data.reset_index(inplace=True)\n",
    "    feature_data = feature_data.rename(columns={k:\"%s_rolling_mean_%d\"%(name_prefix, k) for k in range(13)})\n",
    "\n",
    "    ## std\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).rolling(3).std()\n",
    "\n",
    "    tmp_df = tmp_df.dropna().unstack(level=-1)\n",
    "\n",
    "    if len(stat_dim) == 3:\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "    elif len(stat_dim) == 2:\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "\n",
    "\n",
    "    tmp_df.reset_index(inplace=True)\n",
    "    tmp_df = tmp_df.rename(columns={k:\"%s_rolling_std_%d\"%(name_prefix, k) for k in range(13)})\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "\n",
    "    ## sum\n",
    "    tmp_df = train_sales_data.groupby(stat_dim)[cal_col].apply(lambda x: x.sum()).groupby(stat_dim[:-1]).rolling(3).sum()\n",
    "\n",
    "    tmp_df = tmp_df.dropna().unstack(level=-1)\n",
    "\n",
    "    if len(stat_dim) == 3:\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "    elif len(stat_dim) == 2:\n",
    "        tmp_df.index = tmp_df.index.droplevel(0)\n",
    "\n",
    "    tmp_df.reset_index(inplace=True)\n",
    "    tmp_df = tmp_df.rename(columns={k:\"%s_rolling_sum_%d\"%(name_prefix, k) for k in range(13)})\n",
    "\n",
    "    feature_data = pd.merge(feature_data, tmp_df, on=stat_dim[:-1], how=\"left\")\n",
    "    return feature_data\n",
    "\n",
    "\n",
    "# cal_basic_fea\n",
    "model2type = train_sales[[\"model\", \"bodyType\"]].drop_duplicates().set_index(\"model\").to_dict()[\"bodyType\"]\n",
    "# 城市+车\n",
    "train_basic_fea = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"model\", \"regMonth\"], \"train\")\n",
    "# 城市\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"regMonth\"], \"train\")\n",
    "train_basic_fea = pd.merge(train_basic_fea, tmp_df, on=\"adcode\", how=\"left\")\n",
    "# 车\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"model\", \"regMonth\"], \"train\")\n",
    "train_basic_fea = pd.merge(train_basic_fea, tmp_df, on=\"model\", how=\"left\")\n",
    "# 城市+车型\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"bodyType\", \"regMonth\"], \"train\")\n",
    "train_basic_fea[\"bodyType\"] = train_basic_fea.model.apply(lambda x: model2type[x])\n",
    "train_basic_fea = pd.merge(train_basic_fea, tmp_df, on=[\"adcode\", \"bodyType\"], how=\"left\")\n",
    "\n",
    "\n",
    "# cal_windows_fea\n",
    "# 城市+车\n",
    "train_windows_fea = cal_windows_fea(train_sales, cal_col=\"salesVolume\", stat_dim=[\"adcode\", \"model\", \"regMonth\"], data_type=\"train\")\n",
    "# 城市\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"adcode\", \"regMonth\"], \"train\")\n",
    "train_windows_fea = pd.merge(train_windows_fea, tmp_df, on=\"adcode\", how=\"left\")\n",
    "# 车\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"model\", \"regMonth\"], \"train\")\n",
    "train_windows_fea = pd.merge(train_windows_fea, tmp_df, on=\"model\", how=\"left\")\n",
    "# 城市+车型\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"adcode\", \"bodyType\", \"regMonth\"], \"train\")\n",
    "train_windows_fea[\"bodyType\"] = train_windows_fea.model.apply(lambda x: model2type[x])\n",
    "train_windows_fea = pd.merge(train_windows_fea, tmp_df, on=[\"adcode\", \"bodyType\"], how=\"left\")\n",
    "\n",
    "# 合并特征\n",
    "train_data = pd.merge(train_basic_fea, train_windows_fea, on=[\"adcode\", \"model\", \"bodyType\"], how=\"left\")\n",
    "vaild_data = get_data(train_sales, \"test\").groupby([\"adcode\", \"model\"])[\"salesVolume\"].apply(lambda x: pd.DataFrame(np.array(x)).T).reset_index().drop(\"level_2\", axis=1)\n",
    "\n",
    "# 模型\n",
    "le_model = preprocessing.LabelEncoder()\n",
    "le_bodyType = preprocessing.LabelEncoder()\n",
    "\n",
    "le_model.fit(train_data.model)\n",
    "le_bodyType.fit(train_data.bodyType)\n",
    "\n",
    "lgb_params = {\n",
    "    \"num_leaves\":32,\n",
    "    \"reg_alpha\":1,\n",
    "    \"reg_lambda\":0.1,\n",
    "    \"objective\":'mse',\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"min_child_samples\":5,\n",
    "    \"random_state\":random.randint(100, 10000),\n",
    "    \"n_estimators\":5000,\n",
    "    \"subsample\":0.8,\n",
    "    \"colsample_bytree\":0.8\n",
    "}\n",
    "\n",
    "df_train_columns = [c for c in train_data.columns if c not in []]\n",
    "cate_fea = [\"adcode\", \"model\", \"bodyType\"]\n",
    "train_data.model = le_model.transform(train_data.model)\n",
    "train_data.bodyType = le_bodyType.transform(train_data.bodyType)\n",
    "print(train_data.shape)\n",
    "print(len(df_train_columns))\n",
    "\n",
    "\n",
    "y_score = []    # 交叉验证\n",
    "cv_pred = []    # 各折的预测值\n",
    "predictions = 0\n",
    "feature_importance_df = pd.DataFrame()\n",
    "skf = KFold(n_splits=5, random_state=random.randint(100, 10000), shuffle=True)\n",
    "\n",
    "\n",
    "label_0 = vaild_data[0]\n",
    "label_1 = vaild_data[1]\n",
    "label_2 = vaild_data[2]\n",
    "label_3 = vaild_data[3]\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_data, label_0)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][df_train_columns], label=label_0.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][df_train_columns], label=label_0.iloc[val_idx])\n",
    "\n",
    "    result_df = train_data.iloc[val_idx][[\"adcode\", \"model\"]]\n",
    "\n",
    "    gbm_1 = lgb.train(lgb_params,\n",
    "                    trn_data,\n",
    "                    # init_model=gbm,\n",
    "                    num_boost_round=150000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose_eval=200,\n",
    "                    categorical_feature=cate_fea)     # 训练\n",
    "\n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][df_train_columns], label=label_1.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][df_train_columns], label=label_1.iloc[val_idx])\n",
    "\n",
    "    gbm_2 = lgb.train(lgb_params,\n",
    "                    trn_data,\n",
    "                    # init_model=gbm,\n",
    "                    num_boost_round=150000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose_eval=200,\n",
    "                    categorical_feature=cate_fea)     # 训练\n",
    "\n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][df_train_columns], label=label_2.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][df_train_columns], label=label_2.iloc[val_idx])\n",
    "\n",
    "    gbm_3 = lgb.train(lgb_params,\n",
    "                    trn_data,\n",
    "                    # init_model=gbm,\n",
    "                    num_boost_round=150000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose_eval=200,\n",
    "                    categorical_feature=cate_fea)     # 训练\n",
    "\n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][df_train_columns], label=label_3.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][df_train_columns], label=label_3.iloc[val_idx])\n",
    "\n",
    "    gbm_4 = lgb.train(lgb_params,\n",
    "                    trn_data,\n",
    "                    # init_model=gbm,\n",
    "                    num_boost_round=150000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose_eval=200,\n",
    "                    categorical_feature=cate_fea)     # 训练\n",
    "\n",
    "    result_df[\"y_pred_1\"] = gbm_1.predict(train_data.iloc[val_idx][df_train_columns])\n",
    "    result_df[\"y_pred_2\"] = gbm_2.predict(train_data.iloc[val_idx][df_train_columns])\n",
    "    result_df[\"y_pred_3\"] = gbm_3.predict(train_data.iloc[val_idx][df_train_columns])\n",
    "    result_df[\"y_pred_4\"] = gbm_4.predict(train_data.iloc[val_idx][df_train_columns])\n",
    "    # break\n",
    "\n",
    "result_df[\"y_true_1\"] = label_0\n",
    "result_df[\"y_true_2\"] = label_1\n",
    "result_df[\"y_true_3\"] = label_2\n",
    "result_df[\"y_true_4\"] = label_3\n",
    "\n",
    "# 预测\n",
    "# 基础特征\n",
    "# 城市+车\n",
    "test_basic_fea = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"model\", \"regMonth\"], \"test\")\n",
    "# 城市\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"regMonth\"], \"test\")\n",
    "test_basic_fea = pd.merge(test_basic_fea, tmp_df, on=\"adcode\", how=\"left\")\n",
    "# 车\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"model\", \"regMonth\"], \"test\")\n",
    "test_basic_fea = pd.merge(test_basic_fea, tmp_df, on=\"model\", how=\"left\")\n",
    "# 城市+车型\n",
    "tmp_df = cal_basic_fea(train_sales, \"salesVolume\", [\"adcode\", \"bodyType\", \"regMonth\"], \"test\")\n",
    "test_basic_fea[\"bodyType\"] = test_basic_fea.model.apply(lambda x: model2type[x])\n",
    "test_basic_fea = pd.merge(test_basic_fea, tmp_df, on=[\"adcode\", \"bodyType\"], how=\"left\")\n",
    "\n",
    "# 滑窗特征\n",
    "# 城市+车\n",
    "test_windows_fea = cal_windows_fea(train_sales, cal_col=\"salesVolume\", stat_dim=[\"adcode\", \"model\", \"regMonth\"], data_type=\"test\")\n",
    "# 城市\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"adcode\", \"regMonth\"], \"test\")\n",
    "test_windows_fea = pd.merge(test_windows_fea, tmp_df, on=\"adcode\", how=\"left\")\n",
    "# 车\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"model\", \"regMonth\"], \"test\")\n",
    "test_windows_fea = pd.merge(test_windows_fea, tmp_df, on=\"model\", how=\"left\")\n",
    "# 城市+车型\n",
    "tmp_df = cal_windows_fea(train_sales, \"salesVolume\", [\"adcode\", \"bodyType\", \"regMonth\"], \"test\")\n",
    "test_windows_fea[\"bodyType\"] = test_windows_fea.model.apply(lambda x: model2type[x])\n",
    "test_windows_fea = pd.merge(test_windows_fea, tmp_df, on=[\"adcode\", \"bodyType\"], how=\"left\")\n",
    "\n",
    "# 合并\n",
    "test_data = pd.merge(test_basic_fea, test_windows_fea, on=[\"adcode\", \"model\", \"bodyType\"], how=\"left\")\n",
    "\n",
    "test_data.model = le_model.transform(test_data.model)\n",
    "test_data.bodyType = le_bodyType.transform(test_data.bodyType)\n",
    "\n",
    "y_pred_1 = gbm_1.predict(test_data[df_train_columns])\n",
    "y_pred_2 = gbm_2.predict(test_data[df_train_columns])\n",
    "y_pred_3 = gbm_3.predict(test_data[df_train_columns])\n",
    "y_pred_4 = gbm_4.predict(test_data[df_train_columns])\n",
    "\n",
    "y_pred_1 = (np.e ** y_pred_1 - 1).astype(int)\n",
    "y_pred_2 = (np.e ** y_pred_2 - 1).astype(int)\n",
    "y_pred_3 = (np.e ** y_pred_3 - 1).astype(int)\n",
    "y_pred_4 = (np.e ** y_pred_4 - 1).astype(int)\n",
    "\n",
    "result_df = test_basic_fea[[\"adcode\", \"model\"]]\n",
    "result_df[\"y_pred_1\"] = y_pred_1\n",
    "result_df[\"y_pred_2\"] = y_pred_2\n",
    "result_df[\"y_pred_3\"] = y_pred_3\n",
    "result_df[\"y_pred_4\"] = y_pred_4\n",
    "\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "test_data = test_data.drop(\"forecastVolum\", axis=1)\n",
    "test_data_1 = pd.merge(test_data.loc[test_data.regMonth == 1], result_df[[\"adcode\", \"model\", \"y_pred_1\"]].rename(columns={\"y_pred_1\":\"forecastVolum\"}),\\\n",
    "                       how=\"left\", on=[\"adcode\", \"model\"])\n",
    "test_data_2 = pd.merge(test_data.loc[test_data.regMonth == 2], result_df[[\"adcode\", \"model\", \"y_pred_2\"]].rename(columns={\"y_pred_2\":\"forecastVolum\"}),\\\n",
    "                       how=\"left\", on=[\"adcode\", \"model\"])\n",
    "test_data_3 = pd.merge(test_data.loc[test_data.regMonth == 3], result_df[[\"adcode\", \"model\", \"y_pred_3\"]].rename(columns={\"y_pred_3\":\"forecastVolum\"}),\\\n",
    "                       how=\"left\", on=[\"adcode\", \"model\"])\n",
    "test_data_4 = pd.merge(test_data.loc[test_data.regMonth == 4], result_df[[\"adcode\", \"model\", \"y_pred_4\"]].rename(columns={\"y_pred_4\":\"forecastVolum\"}),\\\n",
    "                       how=\"left\", on=[\"adcode\", \"model\"])\n",
    "result = pd.concat([test_data_1, test_data_2, test_data_3, test_data_4]).reset_index(drop=True)\n",
    "result.forecastVolum = result.forecastVolum.astype(int)\n",
    "result.loc[(result.forecastVolum < 0), \"forecastVolum\"] = 1\n",
    "print((result.forecastVolum < 0 ).sum())\n",
    "result[[\"id\", \"forecastVolum\"]].to_csv(\"../submit/evaluation_public_20190916_lgb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
