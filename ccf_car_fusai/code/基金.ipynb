{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q2(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def get_stat_feature(df_):\n",
    "    df = df_.copy()\n",
    "    stat_feat = []\n",
    "    df['model_adcode'] = df['adcode'] + df['model']\n",
    "    df['model_adcode_mt'] = df['model_adcode'] * 100 + df['mt']\n",
    "\n",
    "    #df1 = df.groupby(['model_adcode'])['salesVolume'].agg(['median', 'std', q1,q2])\n",
    "    #df=pd.merge(df,df1,on=['model_adcode'],how='left')\n",
    "    #stat_feat+=['median','std','q1','q2']\n",
    "\n",
    "\n",
    "    #for col in tqdm(['label','popularity','area_sales_volume','pop_dist']):\n",
    "    for col in tqdm(['label', 'popularity', 'area_sales_volume']):\n",
    "        # shift\n",
    "        for i in [1,2,3,4,5,6,7,8,9]:\n",
    "            stat_feat.append('shift_model_adcode_mt_{}_{}'.format(col,i))\n",
    "            df['model_adcode_mt_{}_{}'.format(col,i)] = df['model_adcode_mt'] + i\n",
    "            df_last = df[~df[col].isnull()].set_index('model_adcode_mt_{}_{}'.format(col,i))\n",
    "            df['shift_model_adcode_mt_{}_{}'.format(col,i)] = df['model_adcode_mt'].map(df_last[col])\n",
    "\n",
    "    return df, stat_feat\n",
    "\n",
    "def score(data, pred='pred_label', label='label', group='model'):\n",
    "    data['pred_label'] = data['pred_label'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    data_agg = data.groupby('model').agg({\n",
    "        pred:  list,\n",
    "        label: [list, 'mean']\n",
    "    }).reset_index()\n",
    "    data_agg.columns = ['_'.join(col).strip() for col in data_agg.columns]\n",
    "    nrmse_score = []\n",
    "    for raw in data_agg[['{0}_list'.format(pred), '{0}_list'.format(label), '{0}_mean'.format(label)]].values:\n",
    "        nrmse_score.append(\n",
    "            mse(raw[0], raw[1]) ** 0.5 / raw[2]\n",
    "        )\n",
    "    print(1 - np.mean(nrmse_score))\n",
    "    return 1 - np.mean(nrmse_score)\n",
    "\n",
    "def get_model_type(train_x,train_y,valid_x,valid_y,m_type='lgb'):\n",
    "    if m_type == 'lgb':\n",
    "        model = lgb.LGBMRegressor(\n",
    "                                num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='mse',\n",
    "                                max_depth=-1, learning_rate=0.05, min_child_samples=5, random_state=2333,\n",
    "                                n_estimators=2000, subsample=0.9, colsample_bytree=0.7,\n",
    "                                )\n",
    "        model.fit(train_x, train_y,\n",
    "              eval_set=[(train_x, train_y),(valid_x, valid_y)],\n",
    "              categorical_feature=cate_feat,\n",
    "              early_stopping_rounds=100, verbose=100)\n",
    "    elif m_type == 'xgb':\n",
    "        model = xgb.XGBRegressor(\n",
    "                                max_depth=5 , learning_rate=0.05, n_estimators=2000,\n",
    "                                objective='reg:gamma', tree_method = 'hist',subsample=0.9,\n",
    "                                colsample_bytree=0.7, min_child_samples=5,eval_metric = 'rmse'\n",
    "                                )\n",
    "        model.fit(train_x, train_y,\n",
    "              eval_set=[(train_x, train_y),(valid_x, valid_y)],\n",
    "              early_stopping_rounds=100, verbose=100)\n",
    "    return model\n",
    "\n",
    "def get_train_model(df_, m, m_type='lgb'):\n",
    "    df = df_.copy()\n",
    "    # 数据集划分\n",
    "    st = 13\n",
    "    all_idx   = (df['mt'].between(st , m-1))\n",
    "    train_idx = (df['mt'].between(st , m-5))\n",
    "    valid_idx = (df['mt'].between(m-4, m-4))\n",
    "    test_idx  = (df['mt'].between(m  , m  ))\n",
    "    print('all_idx  :',st ,m-1)\n",
    "    print('train_idx:',st ,m-5)\n",
    "    print('valid_idx:',m-4,m-4)\n",
    "    print('test_idx :',m  ,m  )\n",
    "    # 最终确认\n",
    "    train_x = df[train_idx][features]\n",
    "    train_y = df[train_idx]['label']\n",
    "    valid_x = df[valid_idx][features]\n",
    "    valid_y = df[valid_idx]['label']\n",
    "    # get model\n",
    "    model = get_model_type(train_x,train_y,valid_x,valid_y,m_type)\n",
    "    # offline\n",
    "    df['pred_label'] = model.predict(df[features])\n",
    "    best_score = score(df[valid_idx])\n",
    "    # online\n",
    "    if m_type == 'lgb':\n",
    "        model.n_estimators = model.best_iteration_ + 100\n",
    "        model.fit(df[all_idx][features], df[all_idx]['label'], categorical_feature=cate_feat)\n",
    "    elif m_type == 'xgb':\n",
    "        model.n_estimators = model.best_iteration + 100\n",
    "        model.fit(df[all_idx][features], df[all_idx]['label'])\n",
    "    df['forecastVolum'] = model.predict(df[features])\n",
    "    print('valid mean:',df[valid_idx]['pred_label'].mean())\n",
    "    print('true  mean:',df[valid_idx]['label'].mean())\n",
    "    print('test  mean:',df[test_idx]['forecastVolum'].mean())\n",
    "    # 阶段结果\n",
    "    sub = df[test_idx][['id']]\n",
    "    sub['forecastVolum'] = df[test_idx]['forecastVolum'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    return sub,df[valid_idx]['pred_label']\n",
    "def quantile_clip(group):\n",
    "    #group.plot()\n",
    "    group[group < group.quantile(.05)] = group.quantile(.05)\n",
    "    group[group > group.quantile(.95)] = group.quantile(.95)\n",
    "    #group.plot()\n",
    "    #plt.show()\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 18.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "all_idx  : 13 24\n",
      "train_idx: 13 20\n",
      "valid_idx: 21 21\n",
      "test_idx : 25 25\n",
      "[0]\tvalidation_0-rmse:757.617\tvalidation_1-rmse:931.125\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:718.08\tvalidation_1-rmse:891.171\n",
      "[200]\tvalidation_0-rmse:229.523\tvalidation_1-rmse:365.122\n",
      "[300]\tvalidation_0-rmse:124.108\tvalidation_1-rmse:252.668\n",
      "[400]\tvalidation_0-rmse:107.776\tvalidation_1-rmse:243.108\n",
      "[500]\tvalidation_0-rmse:100.375\tvalidation_1-rmse:238.056\n",
      "[600]\tvalidation_0-rmse:94.4662\tvalidation_1-rmse:234.209\n",
      "[700]\tvalidation_0-rmse:88.8747\tvalidation_1-rmse:231.041\n",
      "[800]\tvalidation_0-rmse:83.6664\tvalidation_1-rmse:228.863\n",
      "[900]\tvalidation_0-rmse:79.7014\tvalidation_1-rmse:225.99\n",
      "[1000]\tvalidation_0-rmse:75.7866\tvalidation_1-rmse:224.489\n",
      "[1100]\tvalidation_0-rmse:71.9663\tvalidation_1-rmse:223.368\n",
      "[1200]\tvalidation_0-rmse:68.4655\tvalidation_1-rmse:223.076\n",
      "Stopping. Best iteration:\n",
      "[1194]\tvalidation_0-rmse:68.6601\tvalidation_1-rmse:222.943\n",
      "\n",
      "0.7683027711710365\n",
      "valid mean: 512.4066772460938\n",
      "true  mean: 556.6262195121955\n",
      "test  mean: 461.3200988769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "all_idx  : 13 25\n",
      "train_idx: 13 21\n",
      "valid_idx: 22 22\n",
      "test_idx : 26 26\n",
      "[0]\tvalidation_0-rmse:778.807\tvalidation_1-rmse:898.617\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:739.264\tvalidation_1-rmse:859.909\n",
      "[200]\tvalidation_0-rmse:235.811\tvalidation_1-rmse:423.555\n",
      "[300]\tvalidation_0-rmse:123.719\tvalidation_1-rmse:357.222\n",
      "[400]\tvalidation_0-rmse:112.581\tvalidation_1-rmse:351.103\n",
      "[500]\tvalidation_0-rmse:105.382\tvalidation_1-rmse:350.471\n",
      "Stopping. Best iteration:\n",
      "[436]\tvalidation_0-rmse:109.313\tvalidation_1-rmse:349.817\n",
      "\n",
      "0.5938173711118979\n",
      "valid mean: 380.1831970214844\n",
      "true  mean: 531.6218680709541\n",
      "test  mean: 356.3283386230469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 20.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "all_idx  : 13 26\n",
      "train_idx: 13 22\n",
      "valid_idx: 23 23\n",
      "test_idx : 27 27\n",
      "[0]\tvalidation_0-rmse:791.604\tvalidation_1-rmse:956.125\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:752.093\tvalidation_1-rmse:916.346\n",
      "[200]\tvalidation_0-rmse:244.98\tvalidation_1-rmse:427.263\n",
      "[300]\tvalidation_0-rmse:130.936\tvalidation_1-rmse:334.986\n",
      "[400]\tvalidation_0-rmse:115.848\tvalidation_1-rmse:329.047\n",
      "[500]\tvalidation_0-rmse:107.751\tvalidation_1-rmse:325.475\n",
      "[600]\tvalidation_0-rmse:101.715\tvalidation_1-rmse:323.231\n",
      "[700]\tvalidation_0-rmse:98.2076\tvalidation_1-rmse:321.05\n",
      "[800]\tvalidation_0-rmse:93.8661\tvalidation_1-rmse:320.444\n",
      "[900]\tvalidation_0-rmse:89.82\tvalidation_1-rmse:320.186\n",
      "[1000]\tvalidation_0-rmse:86.3518\tvalidation_1-rmse:320.719\n",
      "Stopping. Best iteration:\n",
      "[913]\tvalidation_0-rmse:89.2528\tvalidation_1-rmse:319.882\n",
      "\n",
      "0.6508045630795574\n",
      "valid mean: 443.1093444824219\n",
      "true  mean: 576.4874168514414\n",
      "test  mean: 318.73773193359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n",
      "all_idx  : 13 27\n",
      "train_idx: 13 23\n",
      "valid_idx: 24 24\n",
      "test_idx : 28 28\n",
      "[0]\tvalidation_0-rmse:807.946\tvalidation_1-rmse:1189.55\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:768.364\tvalidation_1-rmse:1148.69\n",
      "[200]\tvalidation_0-rmse:252.186\tvalidation_1-rmse:606.758\n",
      "[300]\tvalidation_0-rmse:134.437\tvalidation_1-rmse:536.715\n",
      "Stopping. Best iteration:\n",
      "[274]\tvalidation_0-rmse:141.189\tvalidation_1-rmse:532.994\n",
      "\n",
      "0.40847847763405454\n",
      "valid mean: 395.0877990722656\n",
      "true  mean: 719.7307926829268\n",
      "test  mean: 285.6610107421875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = '../ccf_car/'\n",
    "    train_sales = pd.read_csv(path + 'train_sales_data.csv')\n",
    "    train_search = pd.read_csv(path + 'train_search_data.csv')\n",
    "    train_user = pd.read_csv(path + 'train_user_reply_data.csv')\n",
    "    evaluation_public = pd.read_csv(path + 'evaluation_public.csv')\n",
    "    submit_example = pd.read_csv(path + 'submit_example.csv')\n",
    "    data = pd.concat([train_sales, evaluation_public], ignore_index=True)\n",
    "    data = data.merge(train_search, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "    data = data.merge(train_user, 'left', on=['model', 'regYear', 'regMonth'])\n",
    "    data['salesVolume'] = data.groupby(['adcode', 'model'])['salesVolume'].transform(quantile_clip)\n",
    "    data['label'] = data['salesVolume']\n",
    "    data['id'] = data['id'].fillna(0).astype(int)\n",
    "    data['bodyType'] = data['model'].map(train_sales.drop_duplicates('model').set_index('model')['bodyType'])\n",
    "    # LabelEncoder\n",
    "    for i in ['bodyType', 'model']:\n",
    "        data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))\n",
    "    data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']\n",
    "\n",
    "    area_sales = {}\n",
    "    for raw in data[['province', 'salesVolume','regMonth','regYear']].values:\n",
    "        province = raw[0]\n",
    "        sales = raw[1]\n",
    "        if pd.isna(sales):\n",
    "            continue\n",
    "        regMonth = raw[2]\n",
    "        regYear = raw[3]\n",
    "        key = province + \"_\" + str(regYear) + \"_\" + str(regMonth)\n",
    "        if area_sales.__contains__(key):\n",
    "            area_sales[key] += sales\n",
    "        else:\n",
    "            area_sales[key] = sales\n",
    "    new_column = []\n",
    "    new_column1 = []\n",
    "    for raw in data[['province', 'salesVolume','regMonth','regYear']].values:\n",
    "        province = raw[0]\n",
    "        sales = raw[1]\n",
    "        if pd.isna(sales):\n",
    "            new_column.append(None)\n",
    "            new_column1.append(None)\n",
    "            continue\n",
    "        regMonth = raw[2]\n",
    "        regYear = raw[3]\n",
    "        key = province + \"_\" + str(regYear) + \"_\" + str(regMonth)\n",
    "        new_column.append(area_sales[key])\n",
    "        new_column1.append(sales/area_sales[key])\n",
    "    data['area_sales_volume'] = new_column\n",
    "\n",
    "    for month in [25, 26, 27, 28]:\n",
    "        m_type = 'xgb'\n",
    "\n",
    "        data_df, stat_feat = get_stat_feature(data)\n",
    "\n",
    "        num_feat = ['regYear'] + stat_feat\n",
    "        cate_feat = ['adcode', 'bodyType', 'model', 'regMonth']\n",
    "        if m_type == 'lgb':\n",
    "            for i in cate_feat:\n",
    "                data_df[i] = data_df[i].astype('category')\n",
    "        elif m_type == 'xgb':\n",
    "            lbl = LabelEncoder()\n",
    "            for i in tqdm(cate_feat):\n",
    "                data_df[i] = lbl.fit_transform(data_df[i].astype(str))\n",
    "\n",
    "        features = num_feat + cate_feat\n",
    "        print(len(features), len(set(features)))\n",
    "\n",
    "        #data_df.to_csv('middle_rst.csv',index=False)\n",
    "        #break\n",
    "        sub, val_pred = get_train_model(data_df, month, m_type)\n",
    "        data.loc[(data.regMonth == (month - 24)) & (data.regYear == 2018), 'salesVolume'] = sub['forecastVolum'].values\n",
    "        data.loc[(data.regMonth == (month - 24)) & (data.regYear == 2018), 'label'] = sub['forecastVolum'].values\n",
    "    sub = data.loc[(data.regMonth >= 1) & (data.regYear == 2018), ['id', 'salesVolume']]\n",
    "    sub.columns = ['id', 'forecastVolum']\n",
    "    sub[['id', 'forecastVolum']].round().astype(int).to_csv('../rst/myx_xgb_quantile_abnormal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[features].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred=data.copy()\n",
    "if __name__ == '__main__':\n",
    "    path = '../ccf_car/'\n",
    "    train_sales = pd.read_csv(path + 'train_sales_data.csv')\n",
    "    train_search = pd.read_csv(path + 'train_search_data.csv')\n",
    "    train_user = pd.read_csv(path + 'train_user_reply_data.csv')\n",
    "    evaluation_public = pd.read_csv(path + 'evaluation_public.csv')\n",
    "    submit_example = pd.read_csv(path + 'submit_example.csv')\n",
    "    data = pd.concat([train_sales, evaluation_public], ignore_index=True)\n",
    "    data = data.merge(train_search, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "    data = data.merge(train_user, 'left', on=['model', 'regYear', 'regMonth'])\n",
    "    data['salesVolume'] = data.groupby(['adcode', 'model'])['salesVolume'].transform(quantile_clip)\n",
    "    data['label'] = data['salesVolume']\n",
    "    data['id'] = data['id'].fillna(0).astype(int)\n",
    "    data['bodyType'] = data['model'].map(train_sales.drop_duplicates('model').set_index('model')['bodyType'])\n",
    "    # LabelEncoder\n",
    "    for i in ['bodyType', 'model']:\n",
    "        data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))\n",
    "    data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']\n",
    "data_true=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_true['label_true']=data_true['label']\n",
    "data_true.drop('label',axis=1,inplace=True)\n",
    "data_true=data_true[['adcode','model','regMonth','regYear','label_true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "毒瘤\n"
     ]
    }
   ],
   "source": [
    "data_final=data_pred.merge(data_true,on=['adcode','model','regMonth','regYear'],how='left')\n",
    "print('毒瘤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adcode</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>forecastVolum</th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>province</th>\n",
       "      <th>regMonth</th>\n",
       "      <th>regYear</th>\n",
       "      <th>salesVolume</th>\n",
       "      <th>popularity</th>\n",
       "      <th>carCommentVolum</th>\n",
       "      <th>newsReplyVolum</th>\n",
       "      <th>label</th>\n",
       "      <th>mt</th>\n",
       "      <th>area_sales_volume</th>\n",
       "      <th>label_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17823</td>\n",
       "      <td>110000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>北京</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>313.55</td>\n",
       "      <td>5994.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>3703.0</td>\n",
       "      <td>313.55</td>\n",
       "      <td>14</td>\n",
       "      <td>16101.15</td>\n",
       "      <td>313.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45802</td>\n",
       "      <td>610000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2551</td>\n",
       "      <td>53</td>\n",
       "      <td>陕西</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>158.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.00</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>310000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>上海</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>484.00</td>\n",
       "      <td>3392.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>484.00</td>\n",
       "      <td>8</td>\n",
       "      <td>28539.65</td>\n",
       "      <td>484.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42893</td>\n",
       "      <td>150000</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>内蒙古</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>64.00</td>\n",
       "      <td>9366.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>6</td>\n",
       "      <td>13592.05</td>\n",
       "      <td>64.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26514</td>\n",
       "      <td>510000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>四川</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>1101.00</td>\n",
       "      <td>2910.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>2960.0</td>\n",
       "      <td>1101.00</td>\n",
       "      <td>21</td>\n",
       "      <td>57785.95</td>\n",
       "      <td>1101.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10471</td>\n",
       "      <td>230000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>黑龙江</td>\n",
       "      <td>8</td>\n",
       "      <td>2016</td>\n",
       "      <td>271.00</td>\n",
       "      <td>2829.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>271.00</td>\n",
       "      <td>8</td>\n",
       "      <td>23859.20</td>\n",
       "      <td>271.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25768</td>\n",
       "      <td>370000</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>山东</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>1912.00</td>\n",
       "      <td>9253.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>1912.00</td>\n",
       "      <td>20</td>\n",
       "      <td>80442.70</td>\n",
       "      <td>1912.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43372</td>\n",
       "      <td>320000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>江苏</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>973.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>973.00</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33530</td>\n",
       "      <td>330000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>浙江</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>293.00</td>\n",
       "      <td>109165.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11372.0</td>\n",
       "      <td>293.00</td>\n",
       "      <td>3</td>\n",
       "      <td>59911.75</td>\n",
       "      <td>293.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15334</td>\n",
       "      <td>310000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>上海</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>556.60</td>\n",
       "      <td>610.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>556.60</td>\n",
       "      <td>12</td>\n",
       "      <td>39870.10</td>\n",
       "      <td>556.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       adcode  bodyType  forecastVolum    id  model province  regMonth  \\\n",
       "17823  110000         1            NaN     0     30       北京         2   \n",
       "45802  610000         0            NaN  2551     53       陕西         2   \n",
       "10450  310000         0            NaN     0     55       上海         8   \n",
       "42893  150000         3            NaN     0     65      内蒙古         6   \n",
       "26514  510000         0            NaN     0      5       四川         9   \n",
       "10471  230000         0            NaN     0     55      黑龙江         8   \n",
       "25768  370000         2            NaN     0     31       山东         8   \n",
       "43372  320000         1            NaN    77      3       江苏         1   \n",
       "33530  330000         1            NaN     0     71       浙江         3   \n",
       "15334  310000         1            NaN     0     37       上海        12   \n",
       "\n",
       "       regYear  salesVolume  popularity  carCommentVolum  newsReplyVolum  \\\n",
       "17823     2017       313.55      5994.0           1078.0          3703.0   \n",
       "45802     2018       158.00         NaN              NaN             NaN   \n",
       "10450     2016       484.00      3392.0            284.0           328.0   \n",
       "42893     2016        64.00      9366.0              0.0          1006.0   \n",
       "26514     2017      1101.00      2910.0            482.0          2960.0   \n",
       "10471     2016       271.00      2829.0            284.0           328.0   \n",
       "25768     2017      1912.00      9253.0            253.0          1513.0   \n",
       "43372     2018       973.00         NaN              NaN             NaN   \n",
       "33530     2016       293.00    109165.0              2.0         11372.0   \n",
       "15334     2016       556.60       610.0            190.0          1743.0   \n",
       "\n",
       "         label  mt  area_sales_volume  label_true  \n",
       "17823   313.55  14           16101.15      313.55  \n",
       "45802   158.00  26                NaN         NaN  \n",
       "10450   484.00   8           28539.65      484.00  \n",
       "42893    64.00   6           13592.05       64.00  \n",
       "26514  1101.00  21           57785.95     1101.00  \n",
       "10471   271.00   8           23859.20      271.00  \n",
       "25768  1912.00  20           80442.70     1912.00  \n",
       "43372   973.00  25                NaN         NaN  \n",
       "33530   293.00   3           59911.75      293.00  \n",
       "15334   556.60  12           39870.10      556.60  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adcode</th>\n",
       "      <th>model</th>\n",
       "      <th>regMonth</th>\n",
       "      <th>regYear</th>\n",
       "      <th>label_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>310000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>292.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>530000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>452.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>233.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>110000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>408.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>510000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>604.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adcode  model  regMonth  regYear  label_true\n",
       "0  310000      0         1     2016      292.00\n",
       "1  530000      0         1     2016      452.70\n",
       "2  150000      0         1     2016      233.45\n",
       "3  110000      0         1     2016      408.00\n",
       "4  510000      0         1     2016      604.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义lgboost模型\n",
    "def Lgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "    Dtrain = lgb.Dataset(np.array(Xtrain),label);\n",
    "    best_round=params['nrounds'];\n",
    "    clf = lgb.train(params,Dtrain,best_round);\n",
    "      \n",
    "    return clf.predict(Xtest),clf.predict(val),clf.feature_importance()\n",
    "#定义xgboost模型\n",
    "def Xgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "    DMtrain = xgb.DMatrix(np.array(Xtrain),label);\n",
    "    DMtest = xgb.DMatrix(np.array(Xtest));\n",
    "    DMval = xgb.DMatrix(np.array(val));\n",
    "    best_round=params['nrounds'];\n",
    "    clf = xgb.train(params,DMtrain,best_round);\n",
    "    \n",
    "    return clf.predict(DMtest),clf.predict(DMval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../ccf_car/'\n",
    "train_sales = pd.read_csv(path + 'train_sales_data.csv')\n",
    "train_search = pd.read_csv(path + 'train_search_data.csv')\n",
    "train_user = pd.read_csv(path + 'train_user_reply_data.csv')\n",
    "evaluation_public = pd.read_csv(path + 'evaluation_public.csv')\n",
    "submit_example = pd.read_csv(path + 'submit_example.csv')\n",
    "data = pd.concat([train_sales, evaluation_public], ignore_index=True)\n",
    "data = data.merge(train_search, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "data = data.merge(train_user, 'left', on=['model', 'regYear', 'regMonth'])\n",
    "data['salesVolume'] = data.groupby(['adcode', 'model'])['salesVolume'].transform(quantile_clip)\n",
    "data['label'] = data['salesVolume']\n",
    "data['id'] = data['id'].fillna(0).astype(int)\n",
    "data['bodyType'] = data['model'].map(train_sales.drop_duplicates('model').set_index('model')['bodyType'])\n",
    "# LabelEncoder\n",
    "for i in ['bodyType', 'model']:\n",
    "    data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))\n",
    "data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_idx  : 13 24\n",
      "train_idx: 13 20\n",
      "valid_idx: 21 21\n",
      "test_idx : 25 25\n",
      "[15:33:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "df = data_df.copy()\n",
    "# 数据集划分\n",
    "st = 13\n",
    "m=25\n",
    "all_idx   = (df['mt'].between(st , m-1))\n",
    "train_idx = (df['mt'].between(st , m-5))\n",
    "valid_idx = (df['mt'].between(m-4, m-4))\n",
    "test_idx  = (df['mt'].between(m  , m+4))\n",
    "print('all_idx  :',st ,m-1)\n",
    "print('train_idx:',st ,m-5)\n",
    "print('valid_idx:',m-4,m-4)\n",
    "print('test_idx :',m  ,m  )\n",
    "# 最终确认\n",
    "train_x = df[train_idx][features]\n",
    "train_y = df[train_idx]['label']\n",
    "valid_x = df[valid_idx][features]\n",
    "valid_y = df[valid_idx]['label']\n",
    "test_x  = df[test_idx][features]\n",
    "#xgb预测\n",
    "xgb_params = {\n",
    "    #'tree_method':\"gpu_hist\",\n",
    "    'objective': 'reg:linear',\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 1,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree':0.06,\n",
    "    'alpha':50,\n",
    "    'lambda':5,\n",
    "    'nrounds':2100\n",
    "}\n",
    "xgby,xgbval = Xgb_To_Pred(train_x,train_y,valid_x,test_x,xgb_params)\n",
    "#lgb预测\n",
    "lgb_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'metric':'mae',\n",
    "    'seed': 0,\n",
    "    'learning_rate':0.04,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.5,\n",
    "    'lambda_l1':1,\n",
    "    'nrounds':900\n",
    "}\n",
    "lgby,lgbval,q = Lgb_To_Pred(train_x,train_y,valid_x,test_x,lgb_params)\n",
    "## 融合第一层预测结果和第二层特征，生成第二层训练、测试集\n",
    "final_train_x=pd.DataFrame()\n",
    "final_train_y=pd.DataFrame()\n",
    "final_test_x=pd.DataFrame()\n",
    "final_train_x['xgbval']=xgbval\n",
    "final_train_x['lgbval']=lgbval\n",
    "final_train_y=valid_y\n",
    "final_test_x['xgby']=xgby\n",
    "final_test_x['lgby']=lgby\n",
    "\n",
    "# # 开始第二层训练、预测\n",
    "lgb2_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'seed':0,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.8,\n",
    "    'nrounds':1400\n",
    "}\n",
    "\n",
    "\n",
    "y_pred,yval,q = Lgb_To_Pred(final_train_x,final_train_y,final_train_x,final_test_x,lgb2_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 259.436715  ,  283.07301227,  137.14633132,  249.32370713,\n",
       "        399.72283201,  137.67707059,  392.17984225,  137.14633132,\n",
       "       2826.68827283,  350.97233976,  633.81060282,  137.10812464,\n",
       "        278.80952085,  279.34026012,  586.69971917])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst=submit_example.copy()\n",
    "rst['forecastVolum']=y_pred.round().astype(int)\n",
    "rst['forecastVolum']=rst['forecastVolum'].apply(lambda x: 0 if x < 0 else x)\n",
    "rst.to_csv('../rst/real_stacking.csv',index=False)\n",
    "# 0.49281591000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([214.59834, 259.77722, 159.94762, 239.67267, 425.10458],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgby[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adcode</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>forecastVolum</th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>province</th>\n",
       "      <th>regMonth</th>\n",
       "      <th>regYear</th>\n",
       "      <th>salesVolume</th>\n",
       "      <th>popularity</th>\n",
       "      <th>carCommentVolum</th>\n",
       "      <th>newsReplyVolum</th>\n",
       "      <th>label</th>\n",
       "      <th>mt</th>\n",
       "      <th>area_sales_volume</th>\n",
       "      <th>model_adcode</th>\n",
       "      <th>model_adcode_mt</th>\n",
       "      <th>model_adcode_mt_label_1</th>\n",
       "      <th>shift_model_adcode_mt_label_1</th>\n",
       "      <th>model_adcode_mt_label_2</th>\n",
       "      <th>shift_model_adcode_mt_label_2</th>\n",
       "      <th>model_adcode_mt_label_3</th>\n",
       "      <th>shift_model_adcode_mt_label_3</th>\n",
       "      <th>model_adcode_mt_label_4</th>\n",
       "      <th>shift_model_adcode_mt_label_4</th>\n",
       "      <th>model_adcode_mt_label_5</th>\n",
       "      <th>shift_model_adcode_mt_label_5</th>\n",
       "      <th>model_adcode_mt_label_6</th>\n",
       "      <th>shift_model_adcode_mt_label_6</th>\n",
       "      <th>model_adcode_mt_label_7</th>\n",
       "      <th>shift_model_adcode_mt_label_7</th>\n",
       "      <th>model_adcode_mt_label_8</th>\n",
       "      <th>shift_model_adcode_mt_label_8</th>\n",
       "      <th>model_adcode_mt_label_9</th>\n",
       "      <th>shift_model_adcode_mt_label_9</th>\n",
       "      <th>model_adcode_mt_popularity_1</th>\n",
       "      <th>shift_model_adcode_mt_popularity_1</th>\n",
       "      <th>model_adcode_mt_popularity_2</th>\n",
       "      <th>shift_model_adcode_mt_popularity_2</th>\n",
       "      <th>model_adcode_mt_popularity_3</th>\n",
       "      <th>shift_model_adcode_mt_popularity_3</th>\n",
       "      <th>model_adcode_mt_popularity_4</th>\n",
       "      <th>shift_model_adcode_mt_popularity_4</th>\n",
       "      <th>model_adcode_mt_popularity_5</th>\n",
       "      <th>shift_model_adcode_mt_popularity_5</th>\n",
       "      <th>model_adcode_mt_popularity_6</th>\n",
       "      <th>shift_model_adcode_mt_popularity_6</th>\n",
       "      <th>model_adcode_mt_popularity_7</th>\n",
       "      <th>shift_model_adcode_mt_popularity_7</th>\n",
       "      <th>model_adcode_mt_popularity_8</th>\n",
       "      <th>shift_model_adcode_mt_popularity_8</th>\n",
       "      <th>model_adcode_mt_popularity_9</th>\n",
       "      <th>shift_model_adcode_mt_popularity_9</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_1</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_1</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_2</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_2</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_3</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_3</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_4</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_4</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_5</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_5</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_6</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_6</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_7</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_7</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_8</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_8</th>\n",
       "      <th>model_adcode_mt_area_sales_volume_9</th>\n",
       "      <th>shift_model_adcode_mt_area_sales_volume_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>上海</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>292.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>292.00</td>\n",
       "      <td>1</td>\n",
       "      <td>37142.60</td>\n",
       "      <td>310000</td>\n",
       "      <td>31000001</td>\n",
       "      <td>31000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31000010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>云南</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>452.70</td>\n",
       "      <td>1594.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>452.70</td>\n",
       "      <td>1</td>\n",
       "      <td>34749.05</td>\n",
       "      <td>530000</td>\n",
       "      <td>53000001</td>\n",
       "      <td>53000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53000010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>内蒙古</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>233.45</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>233.45</td>\n",
       "      <td>1</td>\n",
       "      <td>23645.40</td>\n",
       "      <td>150000</td>\n",
       "      <td>15000001</td>\n",
       "      <td>15000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>北京</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>408.00</td>\n",
       "      <td>2370.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>408.00</td>\n",
       "      <td>1</td>\n",
       "      <td>40538.85</td>\n",
       "      <td>110000</td>\n",
       "      <td>11000001</td>\n",
       "      <td>11000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>四川</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>604.00</td>\n",
       "      <td>3562.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>604.00</td>\n",
       "      <td>1</td>\n",
       "      <td>68720.10</td>\n",
       "      <td>510000</td>\n",
       "      <td>51000001</td>\n",
       "      <td>51000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51000010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adcode  bodyType  forecastVolum  id  model province  regMonth  regYear  \\\n",
       "0       6         0            NaN   0      0       上海         0     2016   \n",
       "1      20         0            NaN   0      0       云南         0     2016   \n",
       "2       3         0            NaN   0      0      内蒙古         0     2016   \n",
       "3       0         0            NaN   0      0       北京         0     2016   \n",
       "4      19         0            NaN   0      0       四川         0     2016   \n",
       "\n",
       "   salesVolume  popularity  carCommentVolum  newsReplyVolum   label  mt  \\\n",
       "0       292.00      1479.0             11.0           106.0  292.00   1   \n",
       "1       452.70      1594.0             11.0           106.0  452.70   1   \n",
       "2       233.45      1479.0             11.0           106.0  233.45   1   \n",
       "3       408.00      2370.0             11.0           106.0  408.00   1   \n",
       "4       604.00      3562.0             11.0           106.0  604.00   1   \n",
       "\n",
       "   area_sales_volume  model_adcode  model_adcode_mt  model_adcode_mt_label_1  \\\n",
       "0           37142.60        310000         31000001                 31000002   \n",
       "1           34749.05        530000         53000001                 53000002   \n",
       "2           23645.40        150000         15000001                 15000002   \n",
       "3           40538.85        110000         11000001                 11000002   \n",
       "4           68720.10        510000         51000001                 51000002   \n",
       "\n",
       "   shift_model_adcode_mt_label_1  model_adcode_mt_label_2  \\\n",
       "0                            NaN                 31000003   \n",
       "1                            NaN                 53000003   \n",
       "2                            NaN                 15000003   \n",
       "3                            NaN                 11000003   \n",
       "4                            NaN                 51000003   \n",
       "\n",
       "   shift_model_adcode_mt_label_2  model_adcode_mt_label_3  \\\n",
       "0                            NaN                 31000004   \n",
       "1                            NaN                 53000004   \n",
       "2                            NaN                 15000004   \n",
       "3                            NaN                 11000004   \n",
       "4                            NaN                 51000004   \n",
       "\n",
       "   shift_model_adcode_mt_label_3  model_adcode_mt_label_4  \\\n",
       "0                            NaN                 31000005   \n",
       "1                            NaN                 53000005   \n",
       "2                            NaN                 15000005   \n",
       "3                            NaN                 11000005   \n",
       "4                            NaN                 51000005   \n",
       "\n",
       "   shift_model_adcode_mt_label_4  model_adcode_mt_label_5  \\\n",
       "0                            NaN                 31000006   \n",
       "1                            NaN                 53000006   \n",
       "2                            NaN                 15000006   \n",
       "3                            NaN                 11000006   \n",
       "4                            NaN                 51000006   \n",
       "\n",
       "   shift_model_adcode_mt_label_5  model_adcode_mt_label_6  \\\n",
       "0                            NaN                 31000007   \n",
       "1                            NaN                 53000007   \n",
       "2                            NaN                 15000007   \n",
       "3                            NaN                 11000007   \n",
       "4                            NaN                 51000007   \n",
       "\n",
       "   shift_model_adcode_mt_label_6  model_adcode_mt_label_7  \\\n",
       "0                            NaN                 31000008   \n",
       "1                            NaN                 53000008   \n",
       "2                            NaN                 15000008   \n",
       "3                            NaN                 11000008   \n",
       "4                            NaN                 51000008   \n",
       "\n",
       "   shift_model_adcode_mt_label_7  model_adcode_mt_label_8  \\\n",
       "0                            NaN                 31000009   \n",
       "1                            NaN                 53000009   \n",
       "2                            NaN                 15000009   \n",
       "3                            NaN                 11000009   \n",
       "4                            NaN                 51000009   \n",
       "\n",
       "   shift_model_adcode_mt_label_8  model_adcode_mt_label_9  \\\n",
       "0                            NaN                 31000010   \n",
       "1                            NaN                 53000010   \n",
       "2                            NaN                 15000010   \n",
       "3                            NaN                 11000010   \n",
       "4                            NaN                 51000010   \n",
       "\n",
       "   shift_model_adcode_mt_label_9  model_adcode_mt_popularity_1  \\\n",
       "0                            NaN                      31000002   \n",
       "1                            NaN                      53000002   \n",
       "2                            NaN                      15000002   \n",
       "3                            NaN                      11000002   \n",
       "4                            NaN                      51000002   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_1  model_adcode_mt_popularity_2  \\\n",
       "0                                 NaN                      31000003   \n",
       "1                                 NaN                      53000003   \n",
       "2                                 NaN                      15000003   \n",
       "3                                 NaN                      11000003   \n",
       "4                                 NaN                      51000003   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_2  model_adcode_mt_popularity_3  \\\n",
       "0                                 NaN                      31000004   \n",
       "1                                 NaN                      53000004   \n",
       "2                                 NaN                      15000004   \n",
       "3                                 NaN                      11000004   \n",
       "4                                 NaN                      51000004   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_3  model_adcode_mt_popularity_4  \\\n",
       "0                                 NaN                      31000005   \n",
       "1                                 NaN                      53000005   \n",
       "2                                 NaN                      15000005   \n",
       "3                                 NaN                      11000005   \n",
       "4                                 NaN                      51000005   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_4  model_adcode_mt_popularity_5  \\\n",
       "0                                 NaN                      31000006   \n",
       "1                                 NaN                      53000006   \n",
       "2                                 NaN                      15000006   \n",
       "3                                 NaN                      11000006   \n",
       "4                                 NaN                      51000006   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_5  model_adcode_mt_popularity_6  \\\n",
       "0                                 NaN                      31000007   \n",
       "1                                 NaN                      53000007   \n",
       "2                                 NaN                      15000007   \n",
       "3                                 NaN                      11000007   \n",
       "4                                 NaN                      51000007   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_6  model_adcode_mt_popularity_7  \\\n",
       "0                                 NaN                      31000008   \n",
       "1                                 NaN                      53000008   \n",
       "2                                 NaN                      15000008   \n",
       "3                                 NaN                      11000008   \n",
       "4                                 NaN                      51000008   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_7  model_adcode_mt_popularity_8  \\\n",
       "0                                 NaN                      31000009   \n",
       "1                                 NaN                      53000009   \n",
       "2                                 NaN                      15000009   \n",
       "3                                 NaN                      11000009   \n",
       "4                                 NaN                      51000009   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_8  model_adcode_mt_popularity_9  \\\n",
       "0                                 NaN                      31000010   \n",
       "1                                 NaN                      53000010   \n",
       "2                                 NaN                      15000010   \n",
       "3                                 NaN                      11000010   \n",
       "4                                 NaN                      51000010   \n",
       "\n",
       "   shift_model_adcode_mt_popularity_9  model_adcode_mt_area_sales_volume_1  \\\n",
       "0                                 NaN                             31000002   \n",
       "1                                 NaN                             53000002   \n",
       "2                                 NaN                             15000002   \n",
       "3                                 NaN                             11000002   \n",
       "4                                 NaN                             51000002   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_1  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_2  \\\n",
       "0                             31000003   \n",
       "1                             53000003   \n",
       "2                             15000003   \n",
       "3                             11000003   \n",
       "4                             51000003   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_2  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_3  \\\n",
       "0                             31000004   \n",
       "1                             53000004   \n",
       "2                             15000004   \n",
       "3                             11000004   \n",
       "4                             51000004   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_3  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_4  \\\n",
       "0                             31000005   \n",
       "1                             53000005   \n",
       "2                             15000005   \n",
       "3                             11000005   \n",
       "4                             51000005   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_4  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_5  \\\n",
       "0                             31000006   \n",
       "1                             53000006   \n",
       "2                             15000006   \n",
       "3                             11000006   \n",
       "4                             51000006   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_5  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_6  \\\n",
       "0                             31000007   \n",
       "1                             53000007   \n",
       "2                             15000007   \n",
       "3                             11000007   \n",
       "4                             51000007   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_6  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_7  \\\n",
       "0                             31000008   \n",
       "1                             53000008   \n",
       "2                             15000008   \n",
       "3                             11000008   \n",
       "4                             51000008   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_7  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_8  \\\n",
       "0                             31000009   \n",
       "1                             53000009   \n",
       "2                             15000009   \n",
       "3                             11000009   \n",
       "4                             51000009   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_8  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "\n",
       "   model_adcode_mt_area_sales_volume_9  \\\n",
       "0                             31000010   \n",
       "1                             53000010   \n",
       "2                             15000010   \n",
       "3                             11000010   \n",
       "4                             51000010   \n",
       "\n",
       "   shift_model_adcode_mt_area_sales_volume_9  \n",
       "0                                        NaN  \n",
       "1                                        NaN  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "BeginTime = time()\n",
    "\n",
    "#读取数据\n",
    "path = '../ccf_car'\n",
    "\n",
    "test_correlation = pd.read_csv(path+'/test_correlation.csv')\n",
    "train_correlation = pd.read_csv(path+'/train_correlation.csv')\n",
    "all_correlation = pd.merge(train_correlation,test_correlation,how='left')\n",
    "TargetID = all_correlation['Unnamed: 0']\n",
    "\n",
    "\n",
    "test_fund_return =  pd.read_csv(path+'/test_fund_return.csv')\n",
    "train_fund_return =  pd.read_csv(path+'/train_fund_return.csv')\n",
    "all_fund_return = pd.merge(train_fund_return,test_fund_return,how='left')\n",
    "\n",
    "\n",
    "test_fund_benchmark_return =  pd.read_csv(path+'/test_fund_benchmark_return.csv')\n",
    "train_fund_benchmark_return =  pd.read_csv(path+'/train_fund_benchmark_return.csv')\n",
    "all_fund_benchmark_return = pd.merge(train_fund_benchmark_return,test_fund_benchmark_return,how='left')\n",
    "\n",
    "\n",
    "test_index_return = pd.read_csv(path+'/test_index_return.csv',encoding='GBK',index_col=0)\n",
    "train_index_return =  pd.read_csv(path+'/train_index_return.csv',encoding='GBK',index_col=0)\n",
    "index_return = pd.concat([train_index_return,test_index_return],axis=1)\n",
    "\n",
    "#根据TargetID把基金对拆分为两列ID，分别为基金1和基金2 \n",
    "Target1 = TargetID.map(lambda x:x.split('-')[0])\n",
    "Target2 = TargetID.map(lambda x:x.split('-')[1])\n",
    "SplitID = pd.concat([Target1,Target2],axis=1)\n",
    "SplitID.columns = ['Target1','Target2']\n",
    "\n",
    "\n",
    "#根据评分规则，定义验证函数\n",
    "from sklearn.metrics import mean_absolute_error  \n",
    "def model_metrics(ypred,ytrue):\n",
    "    msum = 0;\n",
    "    mcount = 0;\n",
    "    for i in range(len(ypred)):\n",
    "        msum += abs((ypred[i]-ytrue[i]) / (1.5-ytrue[i]));\n",
    "        mcount +=1;\n",
    "    mae = mean_absolute_error(ytrue,ypred);\n",
    "    metrics_result = ((2/(2+mae+msum/mcount))**2);\n",
    "    return metrics_result\n",
    "\n",
    "#定义xgboost模型\n",
    "def Xgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "\n",
    "    DMtrain = xgb.DMatrix(np.array(Xtrain),label);\n",
    "    DMtest = xgb.DMatrix(np.array(Xtest));\n",
    "    DMval = xgb.DMatrix(np.array(val));\n",
    "    \n",
    "    best_round=params['nrounds'];\n",
    "    clf = xgb.train(params,DMtrain,best_round);\n",
    "    \n",
    "    return clf.predict(DMtest),clf.predict(DMval)\n",
    "\n",
    "\n",
    "#定义lgboost模型\n",
    "def Lgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "    \n",
    "    Dtrain = lgb.Dataset(np.array(Xtrain),label);\n",
    "    \n",
    "    best_round=params['nrounds'];\n",
    "    clf = lgb.train(params,Dtrain,best_round);\n",
    "      \n",
    "    return clf.predict( Xtest ),clf.predict( val ),clf.feature_importance()\n",
    "\n",
    "#定义IdData函数：根据输入的数据集和起止时间，提取基金1和基金2的数据作为特征\n",
    "\n",
    "def IdData(DataSet,StartTime,EndTime):\n",
    "    \n",
    "    DataID = DataSet[DataSet.columns[0]]\n",
    "    Data   = DataSet[DataSet.columns[StartTime:EndTime]]\n",
    "    \n",
    "    FundData = pd.concat((DataID,Data),axis=1)\n",
    "    FundData.rename(columns={FundData.columns[0]:\"Target1\"},inplace=True)\n",
    "    Target1  = pd.merge(SplitID,FundData,how = 'left')      \n",
    "    FundData.rename(columns={FundData.columns[0]:\"Target2\"},inplace=True)\n",
    "    Target2 = pd.merge(SplitID,FundData,on = 'Target2',how = 'left')\n",
    "    \n",
    "    Target1 = Target1[Target1.columns[2:]]\n",
    "    Target2 = Target2[Target2.columns[2:]]\n",
    "    Target1.columns=range(0,Target1.shape[1])\n",
    "    Target2.columns=range(0,Target2.shape[1])\n",
    "    return Target1,Target2\n",
    "\n",
    "\n",
    "#从相关性计算结果表中提取与TargetID相对应的数据作为特征\n",
    "#因为相关性计算结果表是n*n的矩阵，我们按顺序取对角线左下区域的相关性数据。\n",
    "def GetCorr(q):\n",
    "    for j in range(test_fund_return.shape[0]):\n",
    "        if j ==0:\n",
    "            trainr = q[j][j+1:];\n",
    "        else:\n",
    "            x = q[j][j+1:];\n",
    "            trainr = np.hstack([trainr,x]);\n",
    "    return trainr\n",
    "\n",
    "\n",
    "#计算各基金对Index的相关性，并计算基金对之间的曼哈顿距离之和作为特征\n",
    "def GetIndexCorr(Data,StartTime,EndTime):\n",
    "    a = pd.concat([Data[Data.columns[StartTime:EndTime]].T,index_return[index_return.columns[StartTime:EndTime]].T],axis=1)\n",
    "    b = a.corr()[-35:]\n",
    "    c = b[b.columns[:-35]].T\n",
    "    d = c.rank(axis=1,ascending=False)\n",
    "    e = pd.concat([all_fund_return['Unnamed: 0'],c],axis=1)\n",
    "    A,B = IdData(e,1,None)\n",
    "    return abs(A-B).sum(axis=1)\n",
    "\n",
    "#计算数据集的平均值，25%、50%、75%分位值，作为特征之一\n",
    "def Describe(data,StartTime,EndTime):\n",
    "    a = data[data.columns[StartTime:EndTime]].T\n",
    "    b= a.mean()\n",
    "    c = a.quantile(0.25)\n",
    "    d = a.quantile(0.5)\n",
    "    e = a.quantile(0.75)\n",
    "    return np.vstack([b,c,d,e]).T\n",
    "\n",
    "\n",
    "#提取第一层训练集特征共5组特征:\n",
    "#1、特征分别为基金对的fund_return相关性\\benchmark_return相关性\\fund_return累计值的相关性\\fund_return累计值的曼哈顿距离\\fund_return相关性\n",
    "\n",
    "def GetFeature(StartTime,EndTime): \n",
    "    \n",
    "    Date = all_fund_return.columns[StartTime:EndTime]\n",
    "    FRData = all_fund_return[Date].T ;\n",
    "    FRCorr = GetCorr(FRData.corr()) ;#计算并提取各基金对的fund_return相关性\n",
    "    FRCumCor = GetCorr(FRData.cumsum(axis=1).corr())#计算并提取各基金对的fund_return累计值的相关性\n",
    "    \n",
    "    BRData = all_fund_benchmark_return[Date].T ;\n",
    "    BRData = BRData.corr() ;\n",
    "    BRCorr = GetCorr(BRData) ;#计算并提取各基金对的benchmark_return相关性\n",
    "    \n",
    "    Target1FR,Target2FR = IdData(all_fund_return,StartTime,EndTime)\n",
    "    A,B = Target1FR.cumsum(axis=1), Target2FR.cumsum(axis=1)\n",
    "    FRCum = abs(A[A.columns[-1]]-B[B.columns[-1]])#计算并提取各基金对fund_return累计值的曼哈顿距离\n",
    "    TargetCor = (Target1FR.T).corrwith(Target2FR.T)#计算并提取各基金对fund_return相关性\n",
    "    \n",
    "    return np.vstack([FRCorr,FRCumCor,BRCorr,FRCum,TargetCor]).T\n",
    "    \n",
    "\n",
    "#第二层训练集特征：\n",
    "#第二层特征为基金对的fund_return的曼哈顿距离求和\n",
    "#定义函数：融合第一层预测结果和第二次训练集特征\n",
    "\n",
    "Feature2date = [5,30,60,90]  #第二层训练集的统计时间段，分别为5天、30天、60天、90天\n",
    "\n",
    "def StackFeature2(date,StackData,StartTime,EndTime):\n",
    "    for i in tqdm(date):\n",
    "        \n",
    "        Target1FR,Target2FR = IdData(all_fund_return,-i+StartTime,EndTime)\n",
    "        \n",
    "        MDTargetFR = abs(Target1FR-Target2FR).sum(axis=1)     #计算基金1、2 fund_return的曼哈顿距离并求和   \n",
    "        \n",
    "        StackData = np.vstack([StackData,MDTargetFR])\n",
    "        \n",
    "    return StackData.T\n",
    "\n",
    "\n",
    "#定义函数：根据给定时间间隔和次数，叠加特征集，并增加一组特征：计算基金对相关性的平均值，25%、50%、75%分位值。\n",
    "\n",
    "def StackFeature(StartTime,EndTime,times):\n",
    "    for i in tqdm(range(times)):        \n",
    "        if i ==0:\n",
    "            xtrain = GetFeature(StartTime,EndTime) ;\n",
    "            TCorrDes = Describe(all_correlation,1,None)#计算基金对相关性的 平均值，25%、50%、75%分位值\n",
    "            xtrain = np.hstack([TCorrDes,xtrain])\n",
    "        else:\n",
    "            DayF = StartTime-day*(i+1)\n",
    "            StackTrain = GetFeature(DayF,EndTime) ;\n",
    "            \n",
    "            xtrain = np.hstack([xtrain,StackTrain]) ;\n",
    "\n",
    "    return xtrain\n",
    "\n",
    "\n",
    "#根据给的的时间段和叠加次数，叠加训练集以增加训练集的数据量\n",
    "\n",
    "def StackTrain(EndTime,Time,long):\n",
    "    for i in range(Time):\n",
    "        \n",
    "        Stacktrain = StackFeature(-day+EndTime,EndTime,times) #生成训练集\n",
    "        StackTarget = all_correlation[all_correlation.columns[EndTime+60-i]] #生成训练集对应的目标集\n",
    "                                      \n",
    "        if i == 0 :\n",
    "            TrainData = Stacktrain\n",
    "            TrainTarget = StackTarget\n",
    "        else:\n",
    "            TrainData = np.vstack([TrainData,Stacktrain])  #叠加训练集\n",
    "            TrainTarget = np.hstack([TrainTarget,StackTarget])  #叠加训练集对应的目标集\n",
    "        \n",
    "    return TrainData,TrainTarget\n",
    "\n",
    "\n",
    "# # 生成第一层训练、预测数据\n",
    "\n",
    "#1、定义训练目标和验证集目标\n",
    "trainday=-62#训练集日期\n",
    "valday=-61#验证集日期\n",
    "testday=-61  #用于线下测试集，用于模型验证，\n",
    "ytrain = all_correlation[all_correlation.columns[trainday+60]] ;\n",
    "test_val1 = all_correlation[all_correlation.columns[valday+60]]\n",
    "test_val2 = all_correlation[all_correlation.columns[testday+60]]#用于线下测试集，用于模型验证，\n",
    "\n",
    "#设定:间隔每20天提取一次FRCorr,FRCumCor,BRCorr,FRCum,FRCorr特征，即0-20，0-40……0-400天的数据，生成训练、验证、测试数据集\n",
    "#加上基金对相关性的 平均值，25%、50%、75%分位值共1004列特征\n",
    "day=20\n",
    "times=20\n",
    "#xtrain = StackFeature(-day+trainday,trainday,times) ;\n",
    "xval1 = StackFeature(-day+valday,valday,times) ;\n",
    "xtest = StackFeature(-day,None,times) ;\n",
    "\n",
    "\n",
    "\n",
    "#叠加训练集以增加训练集的数据量\n",
    "xtrain,ytrain=StackTrain(trainday,10,1)\n",
    "\n",
    "\n",
    "# # 开始第一层训练、预测\n",
    "\n",
    "\n",
    "#xgb预测\n",
    "xgb_params = {\n",
    "    #'tree_method':\"gpu_hist\",\n",
    "    'objective': 'reg:linear',\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 1,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree':0.06,\n",
    "    'alpha':50,\n",
    "    'lambda':5,\n",
    "    'nrounds':2100\n",
    "}\n",
    "\n",
    "xgby,xgbval = Xgb_To_Pred(xtrain,ytrain,xval1,xtest,xgb_params)\n",
    "\n",
    "model_metrics(xgbval,test_val1),model_metrics(xgby,test_val2)\n",
    "\n",
    "#lgb预测\n",
    "lgb_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'metric':'mae',\n",
    "    'seed': 0,\n",
    "    'learning_rate':0.04,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.5,\n",
    "    'lambda_l1':1,\n",
    "    'nrounds':900\n",
    "}\n",
    "lgby,lgbval,q = Lgb_To_Pred(xtrain,ytrain,xval1,xtest,lgb_params)\n",
    "\n",
    "model_metrics(lgbval,test_val1),model_metrics(lgby,test_val2)\n",
    "\n",
    "\n",
    "# # 融合第一层预测结果和第二层特征，生成第二层训练、测试集\n",
    "\n",
    "#第一层预测结果融合\n",
    "strain=np.vstack([xgbval,lgbval]);\n",
    "stest=np.vstack([lgby,lgby]);\n",
    "\n",
    "#第一次预测结果和第二层特征融合\n",
    "strain = StackFeature2(Feature2date,strain,valday,valday)\n",
    "stest = StackFeature2(Feature2date,stest,0,None)\n",
    "\n",
    "\n",
    "# # 开始第二层训练、预测\n",
    "\n",
    "lgbs_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'seed':0,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.8,\n",
    "    'nrounds':1400\n",
    "}\n",
    "\n",
    "\n",
    "y_pred,yval,q = Lgb_To_Pred(strain,test_val1,strain,stest,lgbs_params,)\n",
    "print(\"The prediction had almost complited and It takes about \" + str(time()-BeginTime) + 'second')\n",
    "\n",
    "model_metrics(yval,test_val1),model_metrics(y_pred,test_val2)\n",
    "\n",
    "df = pd.DataFrame({'ID':TargetID,'value':y_pred})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df.to_csv('For The Dream.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
