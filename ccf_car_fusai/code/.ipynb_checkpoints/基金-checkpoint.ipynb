{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "#显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "#显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#设置value的显示长度为100，默认为50\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "def q2(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def get_stat_feature(df_):\n",
    "    df = df_.copy()\n",
    "    stat_feat = []\n",
    "    df['model_adcode'] = df['adcode'] + df['model']\n",
    "    df['model_adcode_mt'] = df['model_adcode'] * 100 + df['mt']\n",
    "\n",
    "    df1 = df.groupby(['model_adcode'])['salesVolume'].agg(['median', 'std', q1,q2])\n",
    "    df=pd.merge(df,df1,on=['model_adcode'],how='left')\n",
    "    stat_feat+=['median','std','q1','q2']\n",
    "\n",
    "\n",
    "    #for col in tqdm(['label','popularity','area_sales_volume','pop_dist']):\n",
    "    for col in tqdm(['label', 'popularity', 'area_sales_volume']):\n",
    "        # shift\n",
    "        for i in [1,2,3,4,5,6,7,8,9]:\n",
    "            stat_feat.append('shift_model_adcode_mt_{}_{}'.format(col,i))\n",
    "            df['model_adcode_mt_{}_{}'.format(col,i)] = df['model_adcode_mt'] + i\n",
    "            df_last = df[~df[col].isnull()].set_index('model_adcode_mt_{}_{}'.format(col,i))\n",
    "            df['shift_model_adcode_mt_{}_{}'.format(col,i)] = df['model_adcode_mt'].map(df_last[col])\n",
    "\n",
    "    return df, stat_feat\n",
    "\n",
    "def score(data, pred='pred_label', label='label', group='model'):\n",
    "    data['pred_label'] = data['pred_label'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    data_agg = data.groupby('model').agg({\n",
    "        pred:  list,\n",
    "        label: [list, 'mean']\n",
    "    }).reset_index()\n",
    "    data_agg.columns = ['_'.join(col).strip() for col in data_agg.columns]\n",
    "    nrmse_score = []\n",
    "    for raw in data_agg[['{0}_list'.format(pred), '{0}_list'.format(label), '{0}_mean'.format(label)]].values:\n",
    "        nrmse_score.append(\n",
    "            mse(raw[0], raw[1]) ** 0.5 / raw[2]\n",
    "        )\n",
    "    print(1 - np.mean(nrmse_score))\n",
    "    return 1 - np.mean(nrmse_score)\n",
    "\n",
    "def get_model_type(train_x,train_y,valid_x,valid_y,m_type='lgb'):\n",
    "    if m_type == 'lgb':\n",
    "        model = lgb.LGBMRegressor(\n",
    "                                num_leaves=2**5-1, reg_alpha=0.25, reg_lambda=0.25, objective='mse',\n",
    "                                max_depth=-1, learning_rate=0.05, min_child_samples=5, random_state=2333,\n",
    "                                n_estimators=2000, subsample=0.9, colsample_bytree=0.7,\n",
    "                                )\n",
    "        model.fit(train_x, train_y,\n",
    "              eval_set=[(train_x, train_y),(valid_x, valid_y)],\n",
    "              categorical_feature=cate_feat,\n",
    "              early_stopping_rounds=100, verbose=100)\n",
    "    elif m_type == 'xgb':\n",
    "        model = xgb.XGBRegressor(\n",
    "                                max_depth=5 , learning_rate=0.05, n_estimators=2000,\n",
    "                                objective='reg:gamma', tree_method = 'hist',subsample=0.9,\n",
    "                                colsample_bytree=0.7, min_child_samples=5,eval_metric = 'rmse'\n",
    "                                )\n",
    "        model.fit(train_x, train_y,\n",
    "              eval_set=[(train_x, train_y),(valid_x, valid_y)],\n",
    "              early_stopping_rounds=100, verbose=100)\n",
    "    return model\n",
    "\n",
    "def get_train_model(df_, m, m_type='lgb'):\n",
    "    df = df_.copy()\n",
    "    # 数据集划分\n",
    "    st = 13\n",
    "    all_idx   = (df['mt'].between(st , m-1))\n",
    "    train_idx = (df['mt'].between(st , m-5))\n",
    "    valid_idx = (df['mt'].between(m-4, m-4))\n",
    "    test_idx  = (df['mt'].between(m  , m  ))\n",
    "    print('all_idx  :',st ,m-1)\n",
    "    print('train_idx:',st ,m-5)\n",
    "    print('valid_idx:',m-4,m-4)\n",
    "    print('test_idx :',m  ,m  )\n",
    "    # 最终确认\n",
    "    train_x = df[train_idx][features]\n",
    "    train_y = df[train_idx]['label']\n",
    "    valid_x = df[valid_idx][features]\n",
    "    valid_y = df[valid_idx]['label']\n",
    "    # get model\n",
    "    model = get_model_type(train_x,train_y,valid_x,valid_y,m_type)\n",
    "    # offline\n",
    "    df['pred_label'] = model.predict(df[features])\n",
    "    best_score = score(df[valid_idx])\n",
    "    # online\n",
    "    if m_type == 'lgb':\n",
    "        model.n_estimators = model.best_iteration_ + 100\n",
    "        model.fit(df[all_idx][features], df[all_idx]['label'], categorical_feature=cate_feat)\n",
    "    elif m_type == 'xgb':\n",
    "        model.n_estimators = model.best_iteration + 100\n",
    "        model.fit(df[all_idx][features], df[all_idx]['label'])\n",
    "    df['forecastVolum'] = model.predict(df[features])\n",
    "    print('valid mean:',df[valid_idx]['pred_label'].mean())\n",
    "    print('true  mean:',df[valid_idx]['label'].mean())\n",
    "    print('test  mean:',df[test_idx]['forecastVolum'].mean())\n",
    "    # 阶段结果\n",
    "    sub = df[test_idx][['id']]\n",
    "    sub['forecastVolum'] = df[test_idx]['forecastVolum'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    return sub,df[valid_idx]['pred_label']\n",
    "def quantile_clip(group):\n",
    "    #group.plot()\n",
    "    group[group < group.quantile(.05)] = group.quantile(.05)\n",
    "    group[group > group.quantile(.95)] = group.quantile(.95)\n",
    "    #group.plot()\n",
    "    #plt.show()\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = '../ccf_car/'\n",
    "    train_sales = pd.read_csv(path + 'train_sales_data.csv')\n",
    "    train_search = pd.read_csv(path + 'train_search_data.csv')\n",
    "    train_user = pd.read_csv(path + 'train_user_reply_data.csv')\n",
    "    evaluation_public = pd.read_csv(path + 'evaluation_public.csv')\n",
    "    submit_example = pd.read_csv(path + 'submit_example.csv')\n",
    "    data = pd.concat([train_sales, evaluation_public], ignore_index=True)\n",
    "    data = data.merge(train_search, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "    data = data.merge(train_user, 'left', on=['model', 'regYear', 'regMonth'])\n",
    "    data['salesVolume'] = data.groupby(['adcode', 'model'])['salesVolume'].transform(quantile_clip)\n",
    "    data['label'] = data['salesVolume']\n",
    "    data['id'] = data['id'].fillna(0).astype(int)\n",
    "    data['bodyType'] = data['model'].map(train_sales.drop_duplicates('model').set_index('model')['bodyType'])\n",
    "    # LabelEncoder\n",
    "    for i in ['bodyType', 'model']:\n",
    "        data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))\n",
    "    data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']\n",
    "\n",
    "    area_sales = {}\n",
    "    for raw in data[['province', 'salesVolume','regMonth','regYear']].values:\n",
    "        province = raw[0]\n",
    "        sales = raw[1]\n",
    "        if pd.isna(sales):\n",
    "            continue\n",
    "        regMonth = raw[2]\n",
    "        regYear = raw[3]\n",
    "        key = province + \"_\" + str(regYear) + \"_\" + str(regMonth)\n",
    "        if area_sales.__contains__(key):\n",
    "            area_sales[key] += sales\n",
    "        else:\n",
    "            area_sales[key] = sales\n",
    "    new_column = []\n",
    "    new_column1 = []\n",
    "    for raw in data[['province', 'salesVolume','regMonth','regYear']].values:\n",
    "        province = raw[0]\n",
    "        sales = raw[1]\n",
    "        if pd.isna(sales):\n",
    "            new_column.append(None)\n",
    "            new_column1.append(None)\n",
    "            continue\n",
    "        regMonth = raw[2]\n",
    "        regYear = raw[3]\n",
    "        key = province + \"_\" + str(regYear) + \"_\" + str(regMonth)\n",
    "        new_column.append(area_sales[key])\n",
    "        new_column1.append(sales/area_sales[key])\n",
    "    data['area_sales_volume'] = new_column\n",
    "\n",
    "    for month in [25, 26, 27, 28]:\n",
    "        m_type = 'xgb'\n",
    "\n",
    "        data_df, stat_feat = get_stat_feature(data)\n",
    "\n",
    "        num_feat = ['regYear'] + stat_feat\n",
    "        cate_feat = ['adcode', 'bodyType', 'model', 'regMonth']\n",
    "        if m_type == 'lgb':\n",
    "            for i in cate_feat:\n",
    "                data_df[i] = data_df[i].astype('category')\n",
    "        elif m_type == 'xgb':\n",
    "            lbl = LabelEncoder()\n",
    "            for i in tqdm(cate_feat):\n",
    "                data_df[i] = lbl.fit_transform(data_df[i].astype(str))\n",
    "\n",
    "        features = num_feat + cate_feat\n",
    "        print(len(features), len(set(features)))\n",
    "\n",
    "        #data_df.to_csv('middle_rst.csv',index=False)\n",
    "        #break\n",
    "        sub, val_pred = get_train_model(data_df, month, m_type)\n",
    "        data.loc[(data.regMonth == (month - 24)) & (data.regYear == 2018), 'salesVolume'] = sub['forecastVolum'].values\n",
    "        data.loc[(data.regMonth == (month - 24)) & (data.regYear == 2018), 'label'] = sub['forecastVolum'].values\n",
    "    sub = data.loc[(data.regMonth >= 1) & (data.regYear == 2018), ['id', 'salesVolume']]\n",
    "    sub.columns = ['id', 'forecastVolum']\n",
    "    sub[['id', 'forecastVolum']].round().astype(int).to_csv('../rst/myx_xgb_quantile.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "BeginTime = time()\n",
    "\n",
    "#读取数据\n",
    "path = '../ccf_car'\n",
    "\n",
    "test_correlation = pd.read_csv(path+'/test_correlation.csv')\n",
    "train_correlation = pd.read_csv(path+'/train_correlation.csv')\n",
    "all_correlation = pd.merge(train_correlation,test_correlation,how='left')\n",
    "TargetID = all_correlation['Unnamed: 0']\n",
    "\n",
    "\n",
    "test_fund_return =  pd.read_csv(path+'/test_fund_return.csv')\n",
    "train_fund_return =  pd.read_csv(path+'/train_fund_return.csv')\n",
    "all_fund_return = pd.merge(train_fund_return,test_fund_return,how='left')\n",
    "\n",
    "\n",
    "test_fund_benchmark_return =  pd.read_csv(path+'/test_fund_benchmark_return.csv')\n",
    "train_fund_benchmark_return =  pd.read_csv(path+'/train_fund_benchmark_return.csv')\n",
    "all_fund_benchmark_return = pd.merge(train_fund_benchmark_return,test_fund_benchmark_return,how='left')\n",
    "\n",
    "\n",
    "test_index_return = pd.read_csv(path+'/test_index_return.csv',encoding='GBK',index_col=0)\n",
    "train_index_return =  pd.read_csv(path+'/train_index_return.csv',encoding='GBK',index_col=0)\n",
    "index_return = pd.concat([train_index_return,test_index_return],axis=1)\n",
    "\n",
    "#根据TargetID把基金对拆分为两列ID，分别为基金1和基金2 \n",
    "Target1 = TargetID.map(lambda x:x.split('-')[0])\n",
    "Target2 = TargetID.map(lambda x:x.split('-')[1])\n",
    "SplitID = pd.concat([Target1,Target2],axis=1)\n",
    "SplitID.columns = ['Target1','Target2']\n",
    "\n",
    "\n",
    "#根据评分规则，定义验证函数\n",
    "from sklearn.metrics import mean_absolute_error  \n",
    "def model_metrics(ypred,ytrue):\n",
    "    msum = 0;\n",
    "    mcount = 0;\n",
    "    for i in range(len(ypred)):\n",
    "        msum += abs((ypred[i]-ytrue[i]) / (1.5-ytrue[i]));\n",
    "        mcount +=1;\n",
    "    mae = mean_absolute_error(ytrue,ypred);\n",
    "    metrics_result = ((2/(2+mae+msum/mcount))**2);\n",
    "    return metrics_result\n",
    "\n",
    "#定义xgboost模型\n",
    "def Xgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "\n",
    "    DMtrain = xgb.DMatrix(np.array(Xtrain),label);\n",
    "    DMtest = xgb.DMatrix(np.array(Xtest));\n",
    "    DMval = xgb.DMatrix(np.array(val));\n",
    "    \n",
    "    best_round=params['nrounds'];\n",
    "    clf = xgb.train(params,DMtrain,best_round);\n",
    "    \n",
    "    return clf.predict(DMtest),clf.predict(DMval)\n",
    "\n",
    "\n",
    "#定义lgboost模型\n",
    "def Lgb_To_Pred(Xtrain,label,val,Xtest,params):\n",
    "    \n",
    "    Dtrain = lgb.Dataset(np.array(Xtrain),label);\n",
    "    \n",
    "    best_round=params['nrounds'];\n",
    "    clf = lgb.train(params,Dtrain,best_round);\n",
    "      \n",
    "    return clf.predict( Xtest ),clf.predict( val ),clf.feature_importance()\n",
    "\n",
    "#定义IdData函数：根据输入的数据集和起止时间，提取基金1和基金2的数据作为特征\n",
    "\n",
    "def IdData(DataSet,StartTime,EndTime):\n",
    "    \n",
    "    DataID = DataSet[DataSet.columns[0]]\n",
    "    Data   = DataSet[DataSet.columns[StartTime:EndTime]]\n",
    "    \n",
    "    FundData = pd.concat((DataID,Data),axis=1)\n",
    "    FundData.rename(columns={FundData.columns[0]:\"Target1\"},inplace=True)\n",
    "    Target1  = pd.merge(SplitID,FundData,how = 'left')      \n",
    "    FundData.rename(columns={FundData.columns[0]:\"Target2\"},inplace=True)\n",
    "    Target2 = pd.merge(SplitID,FundData,on = 'Target2',how = 'left')\n",
    "    \n",
    "    Target1 = Target1[Target1.columns[2:]]\n",
    "    Target2 = Target2[Target2.columns[2:]]\n",
    "    Target1.columns=range(0,Target1.shape[1])\n",
    "    Target2.columns=range(0,Target2.shape[1])\n",
    "    return Target1,Target2\n",
    "\n",
    "\n",
    "#从相关性计算结果表中提取与TargetID相对应的数据作为特征\n",
    "#因为相关性计算结果表是n*n的矩阵，我们按顺序取对角线左下区域的相关性数据。\n",
    "def GetCorr(q):\n",
    "    for j in range(test_fund_return.shape[0]):\n",
    "        if j ==0:\n",
    "            trainr = q[j][j+1:];\n",
    "        else:\n",
    "            x = q[j][j+1:];\n",
    "            trainr = np.hstack([trainr,x]);\n",
    "    return trainr\n",
    "\n",
    "\n",
    "#计算各基金对Index的相关性，并计算基金对之间的曼哈顿距离之和作为特征\n",
    "def GetIndexCorr(Data,StartTime,EndTime):\n",
    "    a = pd.concat([Data[Data.columns[StartTime:EndTime]].T,index_return[index_return.columns[StartTime:EndTime]].T],axis=1)\n",
    "    b = a.corr()[-35:]\n",
    "    c = b[b.columns[:-35]].T\n",
    "    d = c.rank(axis=1,ascending=False)\n",
    "    e = pd.concat([all_fund_return['Unnamed: 0'],c],axis=1)\n",
    "    A,B = IdData(e,1,None)\n",
    "    return abs(A-B).sum(axis=1)\n",
    "\n",
    "#计算数据集的平均值，25%、50%、75%分位值，作为特征之一\n",
    "def Describe(data,StartTime,EndTime):\n",
    "    a = data[data.columns[StartTime:EndTime]].T\n",
    "    b= a.mean()\n",
    "    c = a.quantile(0.25)\n",
    "    d = a.quantile(0.5)\n",
    "    e = a.quantile(0.75)\n",
    "    return np.vstack([b,c,d,e]).T\n",
    "\n",
    "\n",
    "#提取第一层训练集特征共5组特征:\n",
    "#1、特征分别为基金对的fund_return相关性\\benchmark_return相关性\\fund_return累计值的相关性\\fund_return累计值的曼哈顿距离\\fund_return相关性\n",
    "\n",
    "def GetFeature(StartTime,EndTime): \n",
    "    \n",
    "    Date = all_fund_return.columns[StartTime:EndTime]\n",
    "    FRData = all_fund_return[Date].T ;\n",
    "    FRCorr = GetCorr(FRData.corr()) ;#计算并提取各基金对的fund_return相关性\n",
    "    FRCumCor = GetCorr(FRData.cumsum(axis=1).corr())#计算并提取各基金对的fund_return累计值的相关性\n",
    "    \n",
    "    BRData = all_fund_benchmark_return[Date].T ;\n",
    "    BRData = BRData.corr() ;\n",
    "    BRCorr = GetCorr(BRData) ;#计算并提取各基金对的benchmark_return相关性\n",
    "    \n",
    "    Target1FR,Target2FR = IdData(all_fund_return,StartTime,EndTime)\n",
    "    A,B = Target1FR.cumsum(axis=1), Target2FR.cumsum(axis=1)\n",
    "    FRCum = abs(A[A.columns[-1]]-B[B.columns[-1]])#计算并提取各基金对fund_return累计值的曼哈顿距离\n",
    "    TargetCor = (Target1FR.T).corrwith(Target2FR.T)#计算并提取各基金对fund_return相关性\n",
    "    \n",
    "    return np.vstack([FRCorr,FRCumCor,BRCorr,FRCum,TargetCor]).T\n",
    "    \n",
    "\n",
    "#第二层训练集特征：\n",
    "#第二层特征为基金对的fund_return的曼哈顿距离求和\n",
    "#定义函数：融合第一层预测结果和第二次训练集特征\n",
    "\n",
    "Feature2date = [5,30,60,90]  #第二层训练集的统计时间段，分别为5天、30天、60天、90天\n",
    "\n",
    "def StackFeature2(date,StackData,StartTime,EndTime):\n",
    "    for i in tqdm(date):\n",
    "        \n",
    "        Target1FR,Target2FR = IdData(all_fund_return,-i+StartTime,EndTime)\n",
    "        \n",
    "        MDTargetFR = abs(Target1FR-Target2FR).sum(axis=1)     #计算基金1、2 fund_return的曼哈顿距离并求和   \n",
    "        \n",
    "        StackData = np.vstack([StackData,MDTargetFR])\n",
    "        \n",
    "    return StackData.T\n",
    "\n",
    "\n",
    "#定义函数：根据给定时间间隔和次数，叠加特征集，并增加一组特征：计算基金对相关性的平均值，25%、50%、75%分位值。\n",
    "\n",
    "def StackFeature(StartTime,EndTime,times):\n",
    "    for i in tqdm(range(times)):        \n",
    "        if i ==0:\n",
    "            xtrain = GetFeature(StartTime,EndTime) ;\n",
    "            TCorrDes = Describe(all_correlation,1,None)#计算基金对相关性的 平均值，25%、50%、75%分位值\n",
    "            xtrain = np.hstack([TCorrDes,xtrain])\n",
    "        else:\n",
    "            DayF = StartTime-day*(i+1)\n",
    "            StackTrain = GetFeature(DayF,EndTime) ;\n",
    "            \n",
    "            xtrain = np.hstack([xtrain,StackTrain]) ;\n",
    "\n",
    "    return xtrain\n",
    "\n",
    "\n",
    "#根据给的的时间段和叠加次数，叠加训练集以增加训练集的数据量\n",
    "\n",
    "def StackTrain(EndTime,Time,long):\n",
    "    for i in range(Time):\n",
    "        \n",
    "        Stacktrain = StackFeature(-day+EndTime,EndTime,times) #生成训练集\n",
    "        StackTarget = all_correlation[all_correlation.columns[EndTime+60-i]] #生成训练集对应的目标集\n",
    "                                      \n",
    "        if i == 0 :\n",
    "            TrainData = Stacktrain\n",
    "            TrainTarget = StackTarget\n",
    "        else:\n",
    "            TrainData = np.vstack([TrainData,Stacktrain])  #叠加训练集\n",
    "            TrainTarget = np.hstack([TrainTarget,StackTarget])  #叠加训练集对应的目标集\n",
    "        \n",
    "    return TrainData,TrainTarget\n",
    "\n",
    "\n",
    "# # 生成第一层训练、预测数据\n",
    "\n",
    "#1、定义训练目标和验证集目标\n",
    "trainday=-62#训练集日期\n",
    "valday=-61#验证集日期\n",
    "testday=-61  #用于线下测试集，用于模型验证，\n",
    "ytrain = all_correlation[all_correlation.columns[trainday+60]] ;\n",
    "test_val1 = all_correlation[all_correlation.columns[valday+60]]\n",
    "test_val2 = all_correlation[all_correlation.columns[testday+60]]#用于线下测试集，用于模型验证，\n",
    "\n",
    "#设定:间隔每20天提取一次FRCorr,FRCumCor,BRCorr,FRCum,FRCorr特征，即0-20，0-40……0-400天的数据，生成训练、验证、测试数据集\n",
    "#加上基金对相关性的 平均值，25%、50%、75%分位值共1004列特征\n",
    "day=20\n",
    "times=20\n",
    "#xtrain = StackFeature(-day+trainday,trainday,times) ;\n",
    "xval1 = StackFeature(-day+valday,valday,times) ;\n",
    "xtest = StackFeature(-day,None,times) ;\n",
    "\n",
    "\n",
    "\n",
    "#叠加训练集以增加训练集的数据量\n",
    "xtrain,ytrain=StackTrain(trainday,10,1)\n",
    "\n",
    "\n",
    "# # 开始第一层训练、预测\n",
    "\n",
    "\n",
    "#xgb预测\n",
    "xgb_params = {\n",
    "    #'tree_method':\"gpu_hist\",\n",
    "    'objective': 'reg:linear',\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 1,\n",
    "    'subsample':1,\n",
    "    'colsample_bytree':0.06,\n",
    "    'alpha':50,\n",
    "    'lambda':5,\n",
    "    'nrounds':2100\n",
    "}\n",
    "\n",
    "xgby,xgbval = Xgb_To_Pred(xtrain,ytrain,xval1,xtest,xgb_params)\n",
    "\n",
    "model_metrics(xgbval,test_val1),model_metrics(xgby,test_val2)\n",
    "\n",
    "#lgb预测\n",
    "lgb_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'metric':'mae',\n",
    "    'seed': 0,\n",
    "    'learning_rate':0.04,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.5,\n",
    "    'lambda_l1':1,\n",
    "    'nrounds':900\n",
    "}\n",
    "lgby,lgbval,q = Lgb_To_Pred(xtrain,ytrain,xval1,xtest,lgb_params)\n",
    "\n",
    "model_metrics(lgbval,test_val1),model_metrics(lgby,test_val2)\n",
    "\n",
    "\n",
    "# # 融合第一层预测结果和第二层特征，生成第二层训练、测试集\n",
    "\n",
    "#第一层预测结果融合\n",
    "strain=np.vstack([xgbval,lgbval]);\n",
    "stest=np.vstack([lgby,lgby]);\n",
    "\n",
    "#第一次预测结果和第二层特征融合\n",
    "strain = StackFeature2(Feature2date,strain,valday,valday)\n",
    "stest = StackFeature2(Feature2date,stest,0,None)\n",
    "\n",
    "\n",
    "# # 开始第二层训练、预测\n",
    "\n",
    "lgbs_params = {\n",
    "   # 'device':'gpu',\n",
    "    'application':'regression_l1',\n",
    "    'seed':0,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth':1,\n",
    "    'feature_fraction':0.8,\n",
    "    'nrounds':1400\n",
    "}\n",
    "\n",
    "\n",
    "y_pred,yval,q = Lgb_To_Pred(strain,test_val1,strain,stest,lgbs_params,)\n",
    "print(\"The prediction had almost complited and It takes about \" + str(time()-BeginTime) + 'second')\n",
    "\n",
    "model_metrics(yval,test_val1),model_metrics(y_pred,test_val2)\n",
    "\n",
    "df = pd.DataFrame({'ID':TargetID,'value':y_pred})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df.to_csv('For The Dream.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
