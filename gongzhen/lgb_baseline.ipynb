{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './ccf_car'\n",
    "\n",
    "train_sales_data = pd.read_csv(path + '/train_sales_data.csv')\n",
    "train_search_data = pd.read_csv(path + '/train_search_data.csv')\n",
    "train_user_reply_data = pd.read_csv(path + '/train_user_reply_data.csv')\n",
    "\n",
    "test = pd.read_csv(path + '/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_sales_data, test], ignore_index=True)\n",
    "data = data.merge(train_search_data, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
    "data = data.merge(train_user_reply_data, 'left', on=['model', 'regYear', 'regMonth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['salesVolume']\n",
    "data['id'] = data['id'].fillna(0).astype(int)\n",
    "del data['salesVolume'], data['forecastVolum']\n",
    "data['bodyType'] = data['model'].map(train_sales_data.drop_duplicates('model').set_index('model')['bodyType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['bodyType', 'model']:\n",
    "    data[i] = data[i].map(dict(zip(data[i].unique(), range(data[i].nunique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mt'] = (data['regYear'] - 2016) * 12 + data['regMonth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adcode</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>id</th>\n",
       "      <th>model</th>\n",
       "      <th>province</th>\n",
       "      <th>regMonth</th>\n",
       "      <th>regYear</th>\n",
       "      <th>popularity</th>\n",
       "      <th>carCommentVolum</th>\n",
       "      <th>newsReplyVolum</th>\n",
       "      <th>label</th>\n",
       "      <th>mt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>上海</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>云南</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>1594.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>466.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>内蒙古</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>北京</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>2370.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>四川</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>3562.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   adcode  bodyType  id  model province  regMonth  regYear  popularity  \\\n",
       "0  310000         0   0      0       上海         1     2016      1479.0   \n",
       "1  530000         0   0      0       云南         1     2016      1594.0   \n",
       "2  150000         0   0      0      内蒙古         1     2016      1479.0   \n",
       "3  110000         0   0      0       北京         1     2016      2370.0   \n",
       "4  510000         0   0      0       四川         1     2016      3562.0   \n",
       "\n",
       "   carCommentVolum  newsReplyVolum  label  mt  \n",
       "0             11.0           106.0  292.0   1  \n",
       "1             11.0           106.0  466.0   1  \n",
       "2             11.0           106.0  257.0   1  \n",
       "3             11.0           106.0  408.0   1  \n",
       "4             11.0           106.0  610.0   1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_feat = []\n",
    "\n",
    "data['model_adcode'] = data['adcode'] + data['model']\n",
    "data['model_adcode_mt'] = data['model_adcode'] * 100 + data['mt']\n",
    "for i in [11]:\n",
    "    i = i + 1\n",
    "    shift_feat.append('shift_model_adcode_mt_label_{0}'.format(i))\n",
    "    data['model_adcode_mt_{0}'.format(i)] = data['model_adcode_mt'] + i\n",
    "    data_last = data[~data.label.isnull()].set_index('model_adcode_mt_{0}'.format(i))\n",
    "    data['shift_model_adcode_mt_label_{0}'.format(i)] = data['model_adcode_mt'].map(data_last['label'])\n",
    "\n",
    "num_feat = ['regYear'] + shift_feat\n",
    "cate_feat = ['adcode', 'bodyType', 'model', 'regMonth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = num_feat + cate_feat\n",
    "\n",
    "# data['n_label'] = data['label'] / data.groupby('model')['label'].transform('mean')\n",
    "train_idx = (data['mt'] <= 20)\n",
    "\n",
    "valid_idx = (data['mt'].between(21, 24))\n",
    "\n",
    "test_idx = (data['mt'] > 24)\n",
    "\n",
    "data['model_weight'] = data.groupby('model')['label'].transform('mean')\n",
    "data['n_label'] = data['label'] / data['model_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[~test_idx][features]\n",
    "y=data[~test_idx]['n_label']\n",
    "test_X=data[test_idx][features]\n",
    "test_y=data[test_idx]['n_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4325446284994026\n",
      "-0.2861643153264876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "xgb_mdl=xgb.XGBRegressor()\n",
    "lgb_mdl=lgb.LGBMRegressor()\n",
    "print(cross_val_score(xgb_mdl,X,y,cv=5,scoring=\"neg_mean_squared_error\").mean())\n",
    "print(cross_val_score(lgb_mdl,X,y,cv=5,scoring=\"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values in feature set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60bffa05ce441b2a9cd9dff27cac581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=10100, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -0.2005924830269578\n",
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=False, max_features=0.9000000000000001, min_samples_leaf=4, min_samples_split=6, n_estimators=100)\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "\n",
    "tpot = TPOTRegressor(verbosity=2,n_jobs=-1)\n",
    "tpot.fit(X,y)\n",
    "tpot.export('tpot_plain.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data[train_idx][features]\n",
    "train_y = data[train_idx]['n_label']\n",
    "\n",
    "valid_x = data[valid_idx][features]\n",
    "valid_y = data[valid_idx]['n_label']\n",
    "\n",
    "# test_x = data[test_idx][features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(data, pred='pred_label', label='label', group='model'):\n",
    "    data[pred] = data[pred].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "    data_agg = data.groupby('model').agg({\n",
    "        pred: list,\n",
    "        label: [list, 'mean'],\n",
    "\n",
    "    }).reset_index()\n",
    "\n",
    "    data_agg.columns = ['_'.join(col).strip() for col in data_agg.columns]\n",
    "    nrmse_score = []\n",
    "    for raw in data_agg[['{0}_list'.format(pred), '{0}_list'.format(label), '{0}_mean'.format(label)]].values:\n",
    "        nrmse_score.append(\n",
    "            mse(raw[0], raw[1]) ** 0.5 / raw[2]\n",
    "        )\n",
    "    print('scoring:')\n",
    "    print(1 - np.mean(nrmse_score))\n",
    "    return 1 - np.mean(nrmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mse(y, y_pred))\n",
    "\n",
    "def cat_Regressor(train_x,train_y,valid_x,valid_y):\n",
    "    cat_model=ctb.CatBoostRegressor(iterations=2000, learning_rate=0.05,\n",
    "                depth=7, eval_metric='RMSE', cat_features=cate_feat)\n",
    "    cat_model.fit(train_x,train_y,eval_set=[(train_x, train_y), (valid_x, valid_y),\n",
    "            ], early_stopping_rounds=100, verbose=100)\n",
    "    data['cat_pred'] = cat_model.predict(data[features]) * data['model_weight']\n",
    "    best_score = score(data[valid_idx],pred='cat_pred')\n",
    "    return cat_model\n",
    "\n",
    "def xgb_Regressor(train_x, train_y, val_x, val_y):\n",
    "    xgb_model = xgb.XGBRegressor(max_depth=6,\n",
    "        eta=0.02,\n",
    "        objective='reg:linear',\n",
    "        silent=0)\n",
    "\n",
    "    xgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y),\n",
    "            ], early_stopping_rounds=100, verbose=100)\n",
    "    data['xgb_pred'] = xgb_model.predict(data[features]) * data['model_weight']\n",
    "    best_score = score(data[valid_idx],pred='xgb_pred')\n",
    "    return xgb_model\n",
    "    \n",
    "def gboost_Regressor(train_x, train_y, valid_x, valid_y):\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=3600, learning_rate=0.05,\n",
    "                                   max_depth=6, max_features='sqrt',\n",
    "                                   min_samples_leaf=20, min_samples_split=20, \n",
    "                    loss='huber', random_state =5)\n",
    "    train_x=train_x.fillna(0)\n",
    "    gb_model.fit(train_x, train_y)\n",
    "    data['gb_pred'] = gb_model.predict(data[features].fillna(0)) * data['model_weight']\n",
    "    best_score = score(data[valid_idx],pred='gb_pred')\n",
    "    return gb_model\n",
    "\n",
    "def lgb_Regressor(train_x, train_y, valid_x, valid_y):\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        num_leaves=32, reg_alpha=1, reg_lambda=0.1, objective='mse',\n",
    "        max_depth=-1, learning_rate=0.05, min_child_samples=5, random_state=np.random.randint(1000),\n",
    "        n_estimators=5000, subsample=0.8, colsample_bytree=0.8\n",
    "    )\n",
    "    \n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y),\n",
    "            ], categorical_feature=cate_feat, early_stopping_rounds=100, verbose=100)\n",
    "    \n",
    "    data['lgb_pred'] = lgb_model.predict(data[features]) * data['model_weight']\n",
    "    best_score = score(data[valid_idx],pred='lgb_pred')\n",
    "    lgb_model.n_estimators = 666\n",
    "    return lgb_model\n",
    "\n",
    "def base_model():\n",
    "    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "    lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "    return ENet,lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBRegressor开始训练...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's l2: 0.176296\tvalid_1's l2: 0.299142\n",
      "[200]\ttraining's l2: 0.109714\tvalid_1's l2: 0.263745\n",
      "[300]\ttraining's l2: 0.0846756\tvalid_1's l2: 0.261737\n",
      "[400]\ttraining's l2: 0.0697191\tvalid_1's l2: 0.258047\n",
      "[500]\ttraining's l2: 0.0618694\tvalid_1's l2: 0.258151\n",
      "Early stopping, best iteration is:\n",
      "[420]\ttraining's l2: 0.0676747\tvalid_1's l2: 0.257694\n",
      "scoring:\n",
      "0.5944981182653273\n"
     ]
    }
   ],
   "source": [
    "print(\"LGBRegressor开始训练...\")\n",
    "lgb_reg= lgb_Regressor(train_x, train_y, valid_x, valid_y)\n",
    "lgb_pred = lgb_reg.predict(data[test_idx][features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOSTRegressor开始训练...\n",
      "[16:29:16] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tvalidation_0-rmse:0.959292\tvalidation_1-rmse:1.14599\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[99]\tvalidation_0-rmse:0.418656\tvalidation_1-rmse:0.519423\n",
      "scoring:\n",
      "0.5586724872501776\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBOOSTRegressor开始训练...\")\n",
    "xgb_reg = xgb_Regressor(train_x, train_y, valid_x, valid_y)\n",
    "xgb_pred = xgb_reg.predict(data[test_idx][features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBOOSTRegressor开始训练...\n",
      "0:\tlearn: 1.2738830\ttest: 1.2727952\ttest1: 1.5364990\tbest: 1.5364990 (0)\ttotal: 132ms\tremaining: 4m 24s\n",
      "100:\tlearn: 0.5246755\ttest: 0.4895168\ttest1: 0.6572954\tbest: 0.6572954 (100)\ttotal: 3.6s\tremaining: 1m 7s\n",
      "200:\tlearn: 0.4790760\ttest: 0.4375386\ttest1: 0.6172712\tbest: 0.6172712 (200)\ttotal: 6.2s\tremaining: 55.5s\n",
      "300:\tlearn: 0.4471853\ttest: 0.4038843\ttest1: 0.6007424\tbest: 0.6007422 (299)\ttotal: 9.54s\tremaining: 53.9s\n",
      "400:\tlearn: 0.4279009\ttest: 0.3838780\ttest1: 0.5939624\tbest: 0.5939624 (400)\ttotal: 13.2s\tremaining: 52.7s\n",
      "500:\tlearn: 0.4079982\ttest: 0.3650929\ttest1: 0.5864255\tbest: 0.5861429 (499)\ttotal: 17.6s\tremaining: 52.7s\n",
      "600:\tlearn: 0.3917887\ttest: 0.3505379\ttest1: 0.5877531\tbest: 0.5849090 (533)\ttotal: 22.4s\tremaining: 52.1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5849089645\n",
      "bestIteration = 533\n",
      "\n",
      "Shrink model to first 534 iterations.\n",
      "scoring:\n",
      "0.5103347207485311\n"
     ]
    }
   ],
   "source": [
    "# 效果太差，直接舍弃\n",
    "print(\"CatBOOSTRegressor开始训练...\")\n",
    "cat_reg = cat_Regressor(train_x, train_y, valid_x, valid_y)\n",
    "cat_pred = cat_reg.predict(data[test_idx][features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDBTRegressor开始训练...\n",
      "scoring:\n",
      "0.600581130327456\n"
     ]
    }
   ],
   "source": [
    "print(\"GDBTRegressor开始训练...\")\n",
    "gb_reg = gboost_Regressor(train_x, train_y, valid_x, valid_y)\n",
    "gb_pred = gb_reg.predict(data[test_idx][features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking...\n",
      "[16:31:34] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:32:41] WARNING: C:/dev/libs/xgboost/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# ####Stacking####\n",
    "print('Stacking...')\n",
    "stacked_averaged_models = StackingRegressor(\n",
    "regressors=[lgb_reg, gb_reg],\n",
    "meta_regressor= xgb_reg\n",
    ")\n",
    "stacked_averaged_models.fit(train_x.fillna(0), train_y.fillna(0))\n",
    "stacked_train_pred = stacked_averaged_models.predict(valid_x.fillna(0))\n",
    "stacked_averaged_models.fit(data[~test_idx][features].fillna(0), data[~test_idx]['n_label'])\n",
    "stacked_pred = stacked_averaged_models.predict(data[test_idx][features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5139839728842504\n",
      "0.4986124939777301\n",
      "0.5139839728842504\n"
     ]
    }
   ],
   "source": [
    "print(rmsle(valid_y, stacked_train_pred))\n",
    "print(rmsle(valid_y, stacked_train_pred*0.55 + gb_pred*0.20 + \n",
    "       lgb_pred*0.25 + xgb_pred*0))\n",
    "print(rmsle(valid_y, stacked_train_pred))\n",
    "ensemble = stacked_pred*0.30 + gb_pred*0.30  + lgb_pred*0.20 + xgb_pred*0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_lgb_rst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = score(data[valid_idx])\n",
    "lgb_model.n_estimators = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model.fit(data[~test_idx][features], data[~test_idx]['n_label'], categorical_feature=cate_feat)\n",
    "data['forecastVolum'] = lgb_model.predict(data[features]) * data['model_weight']\n",
    "sub = data[test_idx][['id']]\n",
    "sub['forecastVolum'] = data[test_idx]['forecastVolum'].apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "sub.to_csv(path + 'lgb_base_0_46.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = data[test_idx][['id']]\n",
    "sub['forecastVolum'] = (data[test_idx]['cat_pred']).apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "sub.to_csv('cat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = data[test_idx][['id']]\n",
    "data['forecastVolum'] = stacked_averaged_models.predict(data[features].fillna(0)) * data['model_weight']\n",
    "sub['forecastVolum'] = (data[test_idx]['forecastVolum']).apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "sub.to_csv('stack_rst.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=pd.read_csv('./rst/stacking_lgb_0_46And0_49_original.csv')\n",
    "today=pd.read_csv('lgb_xgb_cat_gb.csv')\n",
    "best['forecastVolum'] = (best['forecastVolum']*0.6+today['forecastVolum']*0.4).apply(lambda x: 0 if x < 0 else x).round().astype(int)\n",
    "best.to_csv('bestAndToday_rst.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
